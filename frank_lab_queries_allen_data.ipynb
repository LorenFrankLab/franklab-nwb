{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frank Lab NWB query framework (prototype) on Allen Institute data\n",
    "\n",
    "Note: For a more comprehensive introduction to the Frank Lab's query prototypes, see place_field_with_queries.ipynb.  Then come back to this notebook to see how these methods can be applied to Allen Institute data.\n",
    "\n",
    "As one of the goals of NWB is to provide a common schema for neuroscience data across labs, we analyzed data from the Allen Institute using the same query and analysis code originally written to analyze Frank Lab data. See <i>place_field_with_queries.ipynb</i> for a detailed example of how we can do some canonical Frank Lab analyses using PyNWB along with a set of query classes, which we propose as an early prototype of the NWB query framework. Here, we show that these approaches can be flexibly applied to another lab's data.\n",
    "\n",
    "To run these queries, do the following:\n",
    "1. Set up a Python environment with pynwb, numpy, pandas, matplotlib, and network X installed. \n",
    "2. Download the Allen Institute NWB file 'ecephys_session_785402239.nwb'.\n",
    "3. Edit the data_dir and nwb_filename variables in the second cell, such that they point to this NWB file.\n",
    "4. Run the cells in order. We recommend taking time to read the docs and inspect the output along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General\n",
    "import os\n",
    "import pynwb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Local\n",
    "from nwb_query import *\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the NWB File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser('~/Data/Allen_Inst/') \n",
    "nwb_filename = data_dir + 'ecephys_session_785402239.nwb'\n",
    "\n",
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf = io.read()\n",
    "\n",
    "print('Loaded %s' % nwb_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx = 35   # which cluster to look at for single unit analyses\n",
    "stim_of_interest = \"static_gratings\"\n",
    "epoch_of_interest = 1  # 0 indexed \n",
    "speed_threshold = 50 # cm/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: Find the time intervals of all epochs and their stimulus types\n",
    "#### NWBFile (one animal) --> mx2 numpy array (epoch start/stop)\n",
    "In this dataset, an epoch is a single presentation of a stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the start/stop times of the epochs into a numpy array\n",
    "# (Note: we need a method for easily extracting >1 column of a DynamicTable)\n",
    "epoch_ivls_array = np.column_stack((nwbf.epochs['start_time'][:], nwbf.epochs['stop_time'][:]))\n",
    "\n",
    "# Also get the stimulus type (i.e. stimulus_name) of each epoch\n",
    "stim_types = nwbf.epochs['stimulus_name'][:]\n",
    "stim_types[np.where(stim_types == '')[0]] = 'spontaneous_activity'\n",
    "\n",
    "print(\"*** Epochs ***\")\n",
    "print(\"Total # of stimulus presentaions: %d\" % len(epoch_ivls_array))\n",
    "print(\"# of stimulus types: %d\" % len(np.unique(stim_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Compute inter-stimulus intervals\n",
    "The epochs table doesn't explicitly define the inter-stimulus intervals, so we compute those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible ISIs: end of epoch n to start of epoch n+1\n",
    "isi_start = nwbf.epochs['stop_time'][:-2]\n",
    "isi_end = nwbf.epochs['start_time'][1:]\n",
    "\n",
    "# Keep only ISIs with nonzero length\n",
    "isi_ivls_array = []\n",
    "for i in range(len(isi_start)):\n",
    "    if isi_start[i] != isi_end[i]:\n",
    "        isi_ivls_array.append([isi_start[i], isi_end[i]])\n",
    "        \n",
    "\n",
    "# Because we don't seem to have official start/stop times of neural recordings...\n",
    "# ------------------------\n",
    "# ...if necessary, add ISIs for first spike to first epoch start\n",
    "if nwbf.units['spike_times'][cluster_idx][0] != nwbf.epochs['start_time'][0]:\n",
    "    isi_ivls_array.append(\n",
    "        [nwbf.units['spike_times'][cluster_idx][0], nwbf.epochs['start_time'][0]])\n",
    "    \n",
    "# ...and if necessary, add ISIs for last epoch end to last spike\n",
    "if nwbf.epochs['stop_time'][-1] != nwbf.units['spike_times'][cluster_idx][-1]:\n",
    "    isi_ivls_array.append(\n",
    "        [nwbf.epochs['stop_time'][-1], nwbf.units['spike_times'][cluster_idx][-1]])\n",
    "# ------------------------\n",
    "\n",
    "# Convert to numpy array and build a TimeIntervals for queries\n",
    "isi_ivls_array = np.array(isi_ivls_array)\n",
    "\n",
    "# build the TimeIntervals object\n",
    "isi_intervals = TimeIntervals(isi_ivls_array)  \n",
    "\n",
    "print(\"*** Inter-stimulus intervals *** \")\n",
    "print(\"Total # of ISIs: %d\" % len(isi_intervals))\n",
    "print(\"Total duration of ISIs: %0.2f s\\n\" % np.sum(isi_intervals.durations()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: spiking of a given animal and cluster\n",
    "#### NWBFile (one animal) --> PointData (spiking)\n",
    "We extract the spiking of each cluster from the 'units' DynamicTable. This file does not explicitly define the valid intervals (i.e. observation intervals) over which we measured these units' spiking, so we define the valid interval to go from the time of the first spike to the time of the last spike.  This is certainly not precisely correct, and it might be very far off if the unit did not spike in the beginning or end of the experiment.\n",
    "\n",
    "This query results in a PointData object, which represents a process that we observe over some valid intervals (i.e. observation intervals), and which emits an instantaneous event (i.e. delta function) at some set of time points during our observation intervals. For example, a unit's spiking is a set of timestamps (i.e. spike times) occuring within a set of observation intervals. Note that without the observation intervals, we are unable to compute fundamental features of the unit, such as its firing rate. As such, we always keep track of the valid intervals / observation intervals of all time-based data and never represent them as simply lists of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all spike times for this cluster\n",
    "spikes_t = nwbf.units['spike_times'][cluster_idx]\n",
    "\n",
    "# Since we don't have a true start/end time of the recording session, we use\n",
    "# the time of first and last spike as the start/stop valid interval, assuming\n",
    "# no recording dropouts for the duration of the experiment\n",
    "valid_intervals = TimeIntervals(np.array([spikes_t[0], spikes_t[-1]]))\n",
    "\n",
    "# Build a PointData object with the spike times and valid intervals\n",
    "spiking_all = PointData(point_times=spikes_t, valid_intervals=valid_intervals)\n",
    "\n",
    "print('*** Spiking for cluster %s ***' % cluster_idx)\n",
    "print('# of spikes = %d' % spiking_all.point_times.shape[0])\n",
    "print('# of valid intervals = %d' % len(spiking_all.valid_intervals))\n",
    "print('duration of valid intervals = %0.2f s' % np.sum(spiking_all.valid_intervals.durations()))\n",
    "print('SNR = %0.2f' % nwbf.units['snr'][cluster_idx])\n",
    "print('\\n')\n",
    "\n",
    "f = plt.figure(1, figsize=(15, 2))\n",
    "ax1 = f.add_subplot(1, 1, 1)\n",
    "spikeplots = [(spiking_all, 'All spiking')]\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for unit %s (across full experiment)' % cluster_idx)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Queries: Query for cluster spiking in the different epoch types\n",
    "#### PointData (spiking), TimeIntervals (stimulus type) --> PointData (spiking)\n",
    "Query for the unit's spiking only in epochs when the animal experienced each of the different stimulus types. This is our first example of a <i>time query</i>, one of the fundamental query types. In this case we extract the time intervals (array of start/stop times) for all epochs of a given stimulus type, and we query into the unit's spiking data to only get data in these intervals. The intersection of valid intervals is handled automatically behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time queries into the spiking data for cluster {0}\\n\".format(cluster_idx) + \n",
    "      \"yields spiking and valid intervals for each stimulus type...\\n\")\n",
    "\n",
    "# Within the loop, we will add a timeseries to this list for each stimulus type\n",
    "spikeplots = [(spiking_all, 'All spiking')]\n",
    "\n",
    "# Loop over all stimulus types, doing a TIME QUERY for spiking of the cluster during that stimulus type\n",
    "for stim in np.unique(stim_types):\n",
    "\n",
    "    # Get the time intervals of the epochs with this stimulus type\n",
    "    epochs_this_stim = np.where(stim_types==stim)[0]\n",
    "    ivls_array = epoch_ivls_array[epochs_this_stim, :]\n",
    "    query_intervals = TimeIntervals(ivls_array)\n",
    "    \n",
    "    # Execute at TIME QUERY on spiking during epochs of interest\n",
    "    spiking_behav = spiking_all.time_query(query_intervals)\n",
    "    \n",
    "    # append this stimulus type's spiking to the list for later plotting\n",
    "    spikeplots.append((spiking_behav, 'Spikes in\\n%s epochs' % stim))\n",
    "\n",
    "    print('*** Spiking during %s epochs ***' % stim)\n",
    "    print('# of spikes = %d' % spiking_behav.point_times.shape[0])\n",
    "    print('# of valid intervals = %d' % len(spiking_behav.valid_intervals))\n",
    "    print('total duration of valid intervals = %0.2f s' % np.sum(spiking_behav.valid_intervals.durations()))\n",
    "    print('\\n')\n",
    "    \n",
    "# Also analyze the inter-stimulus intervals\n",
    "query_intervals = TimeIntervals(isi_ivls_array)\n",
    "spiking_behav = spiking_all.time_query(query_intervals)  # time query\n",
    "spikeplots.append((spiking_behav, 'Spikes in\\ninter-stim intervals'))\n",
    "print('*** Spiking during inter-stim intervals ***')\n",
    "print('# of spikes = %d' % spiking_behav.point_times.shape[0])\n",
    "print('# of valid intervals = %d' % len(spiking_behav.valid_intervals))\n",
    "print('total duration of valid intervals = %0.2f s' % np.sum(spiking_behav.valid_intervals.durations()))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Plot spiking for all stimulus types\n",
    "fig1 = plt.figure(1, figsize=(15, len(spikeplots)+1))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for unit %s' % cluster_idx)\n",
    "\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Queries: Query for cluster spiking during a single epoch of a given stimulus type\n",
    "#### 1. PointData (all spiking), TimeIntervals (stimulus type) --> PointData (spiking during a stim type)\n",
    "#### 2. PointData (spiking during a stim type), TimeIntervals (epoch) --> PointData (spiking during one stim epoch)\n",
    "Let's look specifically at one stimulus type, and then zoom in on a single epoch of that stimulus type.  Both of these are time queries on PointData objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikeplots = []\n",
    "\n",
    "epochs_this_stim = np.where(stim_types==stim_of_interest)[0]\n",
    "ivls_array = epoch_ivls_array[epochs_this_stim, :]\n",
    "query_intervals = TimeIntervals(ivls_array)\n",
    "    \n",
    "# Execute at TIME QUERY on spiking during epochs of interest\n",
    "spiking_during_stim = spiking_all.time_query(query_intervals)\n",
    "    \n",
    "# append this stimulus type's spiking to the list for later plotting\n",
    "spikeplots.append((spiking_during_stim, 'Spikes in all\\n%s epochs' % stim_of_interest))\n",
    "\n",
    "\n",
    "# Plot spiking\n",
    "fig1 = plt.figure(1, figsize=(15, len(spikeplots)+1))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for unit %s in all %s epochs' % (cluster_idx, stim_of_interest))\n",
    "\n",
    "\n",
    "# Query for spiking in just the first epoch of this stimulus type\n",
    "spiking_first_stim = spiking_all.time_query(query_intervals[epoch_of_interest])\n",
    "\n",
    "fig2 = plt.figure(2, figsize=(15,2))\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple([(spiking_first_stim, 'Spiking in\\n%s epoch %d' % (stim_of_interest, epoch_of_interest+1))], axis=ax2)\n",
    "ax2.set_title(\"Zooming in on %s epoch %d...\" % (stim_of_interest, epoch_of_interest+1))\n",
    "\n",
    "\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query + Time Query: spiking for many clusters during a single epoch of a given stim type\n",
    "#### 1. Dataset Query: NWBFile (one animal) --> n PointData (n units' spiking)\n",
    "#### 2. Time Query: n PointData (n units' spiking) --> n PointData (n units' spiking during one stim epoch)\n",
    "First get the spiking data for a subset of the units, and then do a time query for the units' spiking during a single epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 100\n",
    "print('processing spiking data for %d units in %s epoch %d...\\n' % (num_units, stim_of_interest, epoch_of_interest+1))\n",
    "\n",
    "# ---------------------\n",
    "# 1. Dataset Queries for the first num_units units' spiking\n",
    "# ---------------------\n",
    "spikes_all_clusters = np.array(nwbf.units['spike_times'][:])\n",
    "all_unit_spiking = []\n",
    "for i in range(num_units):\n",
    "    spikes = spikes_all_clusters[i]\n",
    "    valid_start = min(min(spikes[0], isi_ivls_array[0, 0]), epoch_ivls_array[0, 0])\n",
    "    valid_end = max(max(spikes[-1], isi_ivls_array[-1, 1]), epoch_ivls_array[-1, 1])\n",
    "    valid_intervals = TimeIntervals(np.array([valid_start, valid_end]))\n",
    "    spiking = PointData(point_times=spikes, valid_intervals=valid_intervals)\n",
    "    all_unit_spiking.append(spiking)\n",
    "    \n",
    "    \n",
    "# ---------------------\n",
    "# 2. Time Queries for the first num_units units' spiking during a single epoch\n",
    "#    of a given stimulus type (same epoch from previous cell)\n",
    "# ---------------------\n",
    "spikeplots = []\n",
    "for i, unit_spiking in enumerate(all_unit_spiking):\n",
    "    spiking_during_epoch = unit_spiking.time_query(query_intervals[epoch_of_interest])\n",
    "    spikeplots.append((spiking_during_epoch, 'Unit %d' % i))\n",
    "    \n",
    "f = plt.figure(1, figsize=(15, 100))\n",
    "ax1 = f.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for the first %d units in %s epoch %d...' % (num_units, stim_of_interest, epoch_of_interest+1))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: Animal speed across all epochs\n",
    "#### NWBFile (one animal) --> ContinuousData (speed)\n",
    "Extract the animal's speed data. This is our first example of a ContinuousData object, which represents data that are, in theory, a continuous function of time.  For example, the animal theoretically has a speed at all time points, but we only sample it with some regular sampling rate. We represent this as a set of valid intervals (i.e. observation intervals), within which we have pairs of sample times (i.e. timestamps) and samples (i.e. the speed measurements). As with unit spiking, we always keep track of the valid intervals over which it was possible for us to be measuring the animal's behavior, not just lists of timestamps and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_timeseries = nwbf.modules['running']['speed']\n",
    "sample_times = speed_timeseries.timestamps[()]\n",
    "speed_valid_intervals = TimeIntervals(np.array([sample_times[0], sample_times[-1]]))\n",
    "speed_all_epochs = ContinuousData(samples=pd.DataFrame(data=speed_timeseries.data[()], columns=['speed']), \n",
    "                                  sample_times=sample_times, \n",
    "                                  valid_intervals=speed_valid_intervals)\n",
    "\n",
    "f = plt.figure(figsize=(15,2))\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "plot_ContinuousData(speed_all_epochs, axis=ax)\n",
    "plt.ylim(-40, 150)\n",
    "plt.title('Animal running speed (all epochs)', fontsize=14)\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Velocity (m/s)', fontsize=12)\n",
    "\n",
    "l = ax.axvline(query_intervals[epoch_of_interest].to_array()[0][0], color='k', linestyle='--', linewidth=2)\n",
    "ax.axvline(query_intervals[epoch_of_interest].to_array()[0][1], color='k', linestyle='--', linewidth=2)\n",
    "ax.legend([l], ['epoch of interest'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: animal speed in one epoch\n",
    "#### ContinuousData (speed), epoch --> ContinuousData (speed in one epoch)\n",
    "We now do a time query for the animal's speed during a particular epoch of interest, denoted by dotted line in the plot above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_one_epoch = speed_all_epochs.time_query(query_intervals[epoch_of_interest])\n",
    "\n",
    "f = plt.figure(figsize=(15,2))\n",
    "ax1 = f.add_subplot(1, 1, 1)\n",
    "plot_ContinuousData(speed_one_epoch, axis=ax1)\n",
    "plt.title('Animal running speed during %s epoch %d' % (stim_of_interest, epoch_of_interest+1), fontsize=14)\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Velocity (cm/s)', fontsize=12)\n",
    "\n",
    "ax1.axvline(query_intervals[epoch_of_interest].to_array()[0][0], color='k', linestyle='--', linewidth=2)\n",
    "ax1.axvline(query_intervals[epoch_of_interest].to_array()[0][1], color='k', linestyle='--', linewidth=2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Find time intervals where speed > threshold\n",
    "#### ContinuousData (speed), lambda function --> EventData (time periods where animal speed > threshold)\n",
    "We now compute an analysis function on our speed data to find all of the time intervals where the animal was running faster than a threshold. This returns and EventData object, which is our way of representing the occurence of \"events\" (i.e. time intervals) that occur within some observation interval.  This is very similar to PointData--which we used to represent spiking--except instead of just timestamps, here we instead have start/stop times (i.e. time intervals) occuring at various points throughout our observation interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_threshold_fn = lambda x: x > speed_threshold\n",
    "\n",
    "speed_events = speed_one_epoch.filter_intervals(speed_threshold_fn)\n",
    "\n",
    "print('*** Times where speed > {} cm/s ***'.format(speed_threshold))\n",
    "print('# of events = %d' % len(speed_events.event_intervals))\n",
    "print('duration of events = %0.2f s' % np.sum(speed_events.durations()))\n",
    "print('# of valid intervals = %d' % len(speed_events.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(speed_events.valid_durations()))\n",
    "\n",
    "# TODO: Plot continuous speed\n",
    "fig1 = plt.figure(1, figsize=(15.5,1.5))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_EventData(speed_events, axis=ax1)\n",
    "ax1.set_yticks([1])\n",
    "ax1.set_yticklabels(['Run events \\n(speed>threshold)']) # eventually use metadata from PointData object\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: spiking during one epoch, only in intervals where speed > threshold\n",
    "#### PointData (spiking in one epoch), EventData (bouts where speed > threshold) --> PointData (spiking during one epoch during bouts when speed > threshold)\n",
    "Now, we use all of those \"fast running events\" to do a time query into the unit spiking, giving us only the spiking data that occurs within any of those events. Here the imporatance of valid intervals is obvious, as we are intersecting the unit's spiking with potentially thousands of short time intervals (i.e. the \"fast running events\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time query using EventData as the argument (instead of TimeIntervals as we have been doing above)\n",
    "spiking_run = spiking_first_stim.time_query(speed_events)  \n",
    "\n",
    "print('*** Spiking where speed > threshold ***')\n",
    "print('# of spikes = %d' % len(spiking_run.point_times))\n",
    "print('# of intervals = %d' % len(spiking_run.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking_run.valid_intervals.durations()))\n",
    "print()\n",
    "\n",
    "# print(spiking_run)\n",
    "\n",
    "# Plot spiking\n",
    "fig1 = plt.figure(1, figsize=(15,1.5))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple([(spiking_run, 'Spiking during\\nrun events')], axis=ax1)\n",
    "plt.title('Spiking while running > {0} cm/s during one {1} epoch'.format(speed_threshold, stim_of_interest))\n",
    "\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: Spiking of one unit during all drifting gradients epochs\n",
    "#### PointData (spiking all epochs), TimeIntervals (drifting gradient epochs) --> PointData (spiking during drifting gradient epochs)\n",
    "Now let's switch gears and look at specifically at the \"Drifting Gradient\" stimulus. This stimulus is interesting because each epoch (i.e. stimulus presentation) has a particular gradient orientation, so we can look for tuning curves and the like.  In this cell, we first extract all of the drifting gradients epochs; note that there are actually many very short epochs, arranged into three broader \"periods\" (Fig. 1 below). We then zoom in on a subset of those epochs, occurring in the first period of drifiting grating epochs (Fig. 2 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_type = 'drifting_gratings'\n",
    "\n",
    "# Get the time intervals for epochs of this stim type\n",
    "epochs_this_stim = np.where(stim_types==stim_type)[0]\n",
    "query_intervals = TimeIntervals(epoch_ivls_array[epochs_this_stim, :])\n",
    "\n",
    "spikeplots = []\n",
    "\n",
    "# Execute at TIME QUERY on spiking during the time intervals of this stim type\n",
    "spiking_behav = spiking_all.time_query(query_intervals)\n",
    "\n",
    "spikeplots.append((spiking_behav, 'Unit %d' % cluster_idx))\n",
    "\n",
    "print(\"Fig 1. Note the three lighter blue rectangles are made up of many smaller light blue rectangles, which are the individual drifting gradient epochs.\")\n",
    "print(\"Fig 2. You can see this more clearly if we zoom in on just one of those bigger three periods.\\n\")\n",
    "\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15, len(spikeplots)+1))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for unit %s during all %s epochs' % (cluster_idx, stim_type))\n",
    "\n",
    "\n",
    "\n",
    "# Time query\n",
    "first_period = TimeIntervals(np.array([1500, 2500]))\n",
    "spiking_first_period = spiking_behav.time_query(first_period)\n",
    "\n",
    "fig2 = plt.figure(2, figsize=(15, 2))\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple([(spiking_first_period, 'first period of\\ndrifting gradient epochs')], axis=ax2)\n",
    "plt.title('Zooming in on the first period of %s epochs...' % (stim_type))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Queries: One cell's spiking in response to different gradient orientations\n",
    "Now we use a succession of time queries to extract the unit's spiking only during epochs in which particular orientations of the drifiting grating stimulus was presented to the animal (Fig. 1 below).  We can then look at the distributions of spike counts in epochs of each orientation (Fig. 2 below).\n",
    "\n",
    "Note: Time queries on units with many spikes can be currently be pretty slow, because we have to check whether each spike falls within any of the query intervals. You can search for one spike in order log(# query intervals), but without parallelizing over spikes this can still get costly, especially when we start doing dozens of time queries on hundreds of cells, each with tens of thousands of spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the orientations and epoch intervals for each epoch of this stim type\n",
    "all_epochs_ori = nwbf.epochs['Ori'][:]\n",
    "stim_ori = all_epochs_ori[epochs_this_stim]  # orientation for all epochs of this stim type\n",
    "stim_ivls = epoch_ivls_array[epochs_this_stim, :]  # time intervals for all epochs of this stim type\n",
    "\n",
    "# Get the set of all possible orientations (removing 'nan') \n",
    "possible_ori = np.unique(stim_ori)\n",
    "possible_ori = possible_ori[~np.isnan(possible_ori)]\n",
    "\n",
    "spikeplots = []\n",
    "rate_vs_ori = {ori: [] for ori in possible_ori}\n",
    "\n",
    "# For each possible orientation, get the distribution of firing of this cell\n",
    "for i, ori in enumerate(possible_ori):\n",
    "    # Get the time intervals for epochs of this orientation\n",
    "    this_ori_epochs = np.where(stim_ori==ori)[0]\n",
    "    this_ori_ivls = TimeIntervals(stim_ivls[this_ori_epochs, :])\n",
    "    \n",
    "    # Do a Time Query on just the epochs with this orientation\n",
    "    this_ori_spiking = spiking_all.time_query(this_ori_ivls)\n",
    "    spikeplots.append((this_ori_spiking, int(ori)))\n",
    "    \n",
    "    for ivl in this_ori_ivls:\n",
    "        ivl_spiking = this_ori_spiking.time_query(TimeIntervals(ivl))\n",
    "        ivl_spike_count = len(ivl_spiking.point_times)\n",
    "        ivl_length = ivl_spiking.valid_intervals.durations()[0][0]\n",
    "        rate_vs_ori[ori].append(ivl_spike_count)\n",
    "    \n",
    "fig1 = plt.figure(1, figsize=(15, len(spikeplots)+1))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "ax1.set_ylabel('Orientation (degrees)')\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "plt.title('Spiking for unit %s during %s epochs' % (cluster_idx, stim_type))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Spike count vs. orientation\\n(all drifting gradient epochs)')\n",
    "plt.xlabel('Orientation (degrees)')\n",
    "plt.ylabel('# Spikes')\n",
    "ax = plt.subplot(111)\n",
    "plt.boxplot([rate_vs_ori[ori] for ori in possible_ori])\n",
    "ax.set_xticklabels([int(ori) for ori in possible_ori])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

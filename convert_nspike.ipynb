{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "\n",
    "import pynwb\n",
    "import nspike_helpers as ns \n",
    "import query_helpers as qu\n",
    "import fl_apparatus as ap\n",
    "\n",
    "mdates.rcParams.update({'date.autoformatter.microsecond': '%H:%M:%S.%f'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug settings\n",
    "limit_num_of_tets = None # To speed up testing. Set to None to load all tets\n",
    "\n",
    "# Session-specific params\n",
    "# data_dir = '/opt/data46/FrankData/kkay/Bon/'\n",
    "\n",
    "data_dir, data_src = os.path.expanduser('~/Data/FrankData/kkay/Bon'), 'local'\n",
    "# data_dir, data_src = os.path.expanduser('~/Data/FrankData/CRCNS/Bon'), 'CRCNS'\n",
    "\n",
    "anim = 'Bon' \n",
    "day = 5 # below we'll code date as 2006-Jan-'Day'\n",
    "\n",
    "# 'Wall clock' (i.e. actual) date and time of the Nspike time = 0 for this experiment.\n",
    "# NOTE: this is not the true zero_time, as we don't have easy access to that without digging in lab notebooks.\n",
    "# TODO: dig in lab notebooks, at least to get the date of recording.\n",
    "dataset_zero_time = datetime(2006, 1, day, 12, 0, 0, tzinfo=tz.gettz('US/Pacific'))\n",
    "\n",
    "# General params/presets\n",
    "file_create_date = datetime.now(tz.tzlocal())\n",
    "\n",
    "eeg_samprate = 1500.0 # Hz\n",
    "\n",
    "eeg_subdir = \"EEG\"\n",
    "epochs_file = \"times.mat\"\n",
    "tetinfo_file = \"tetinfo.mat\"\n",
    "NSpike_timestamps_per_sec = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-11 10:30:09.527744-07:00\n",
      "2006-01-05 12:00:00-08:00\n"
     ]
    }
   ],
   "source": [
    "print(file_create_date)\n",
    "print(dataset_zero_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse inputs and create NWBfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_str = '%02d' % day\n",
    "\n",
    "nwb_filename = anim + day_str + '_test.nwb'\n",
    "\n",
    "# check the input arguments\n",
    "if not os.path.exists(data_dir):\n",
    "        print('Error: data_dir %s does not exist' % data_dir)\n",
    "        exit(-1)\n",
    "\n",
    "# get filename prefix and file locations\n",
    "prefix = anim.lower()\n",
    "eeg_path = os.path.join(data_dir, eeg_subdir)\n",
    "\n",
    "# Calculate the POSIX timestamp when Nspike clock = 0 (seconds)\n",
    "NSpike_posixtime_offset = dataset_zero_time.timestamp()\n",
    "\n",
    "# We'll still store NSpike/Trodes zero time in nwbfile.session_start_time, \n",
    "# so that we can recreate the experimental timestamps, even though that\n",
    "# violates the definition of session_start_time as zero time. We need some\n",
    "# other way to indicate that the timestamps are POSIX (easy: if they are over\n",
    "# a trillion, then they're POSIX!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf = pynwb.NWBFile(\n",
    "           session_description='Converted NSpike data from %s' % data_dir,\n",
    "           identifier=anim+day_str,\n",
    "           session_start_time=dataset_zero_time,\n",
    "           file_create_date=file_create_date,\n",
    "           lab='Frank Laboratory',\n",
    "           experimenter='Mattias Karlsson',\n",
    "           institution='UCSF',\n",
    "           experiment_description='Recordings from awake behaving rat',\n",
    "           session_id=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create position, direction and speed\n",
    "position_list = []\n",
    "pos = pynwb.behavior.Position(spatial_series=position_list, \n",
    "                              name='Position')\n",
    "\n",
    "direction_list = []\n",
    "dir = pynwb.behavior.CompassDirection(spatial_series=direction_list, \n",
    "                                      name='Head Direction')\n",
    "\n",
    "speed_list = []\n",
    "speed = pynwb.behavior.BehavioralTimeSeries(time_series=speed_list, \n",
    "                                            name='Speed')\n",
    "\n",
    "# NOTE that day_inds is 0 based\n",
    "# time_list = {}\n",
    "# nwb_epoch = {}\n",
    "pos_files = ns.get_files_by_day(data_dir, prefix, 'pos')\n",
    "task_files = ns.get_files_by_day(data_dir, prefix, 'task')\n",
    "\n",
    "if data_src == 'local':\n",
    "    linpos_files = ns.get_files_by_day(data_dir, prefix, 'linpos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat_ff(task_files[day], 'task')\n",
    "task_struct = mat[day]\n",
    "# find the pos file for this day and load it\n",
    "\n",
    "mat = ns.loadmat_ff(pos_files[day], 'pos')\n",
    "pos_struct = mat[day]\n",
    "\n",
    "# CRCNS data does not presently have linpos files\n",
    "if data_src == 'local':\n",
    "    mat = ns.loadmat_ff(linpos_files[day], 'linpos')\n",
    "    linpos_struct = mat[day]\n",
    "    # Fill in None for epochs that have no linpos data\n",
    "    for k in task_struct.keys():\n",
    "        if k not in linpos_struct:\n",
    "            linpos_struct[k] = None\n",
    "\n",
    "# compile a list of time intervals in the pos struct (i.e. start/stop times of position data)\n",
    "# currently this is how epochs are defined in Matlab. The task struct, which contains metadata about\n",
    "# epochs, does not itself have start/stop times of the epoch.\n",
    "pos_time_ivls = []\n",
    "\n",
    "# Assume field order: (time,x,y,dir,vel)\n",
    "(time_idx, x_idx, y_idx, dir_idx, vel_idx) = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num, pos_epoch in pos_struct.items():\n",
    "\n",
    "    # convert times to POSIX time\n",
    "    timestamps = pos_epoch['data'][:,time_idx] + NSpike_posixtime_offset\n",
    "\n",
    "    # TODO: create a shared TimeSeries for timestamps, across all behavioral timeseries\n",
    "    # ?? timestamps_obj = pynwb.TimeSeries(timestamps=timestamps...)\n",
    "\n",
    "    # collect times of epoch start and end\n",
    "    pos_time_ivls.append([timestamps[0], timestamps[-1]])\n",
    "\n",
    "    m_per_pixel = pos_epoch['cmperpixel'][0,0]/100 # NWB wants meters per pixel\n",
    "\n",
    "    # we can also create new SpatialSeries for the position, direction and velocity information\n",
    "    #NOTE: Each new spatial series has to have a unique name.\n",
    "    pos.create_spatial_series(name='Position d%d e%d' % (day, epoch_num), \n",
    "                              timestamps = timestamps,\n",
    "                              data=pos_epoch['data'][:, (x_idx, y_idx)] * m_per_pixel,\n",
    "                              reference_frame='corner of video frame',\n",
    "                              #conversion=m_per_pixel,\n",
    "                              #unit='m'\n",
    "                              ) # *after* conversion\n",
    "\n",
    "    dir.create_spatial_series(name='Head Direction d%d e%d'% (day, epoch_num), \n",
    "                              timestamps=timestamps,\n",
    "                              data=pos_epoch['data'][:, dir_idx],\n",
    "                              reference_frame='0=facing top of video frame (?), positive clockwise (?)',\n",
    "                              #unit='radians'\n",
    "                              )\n",
    "\n",
    "    speed.create_timeseries(name='Speed d%d e%d' % (day, epoch_num),\n",
    "                             timestamps=timestamps,\n",
    "                             data=pos_epoch['data'][:, vel_idx] * m_per_pixel,\n",
    "                             unit='m/s',\n",
    "                             #conversion=m_per_pixel,\n",
    "                             description='smoothed movement speed estimate')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Speed <class 'pynwb.behavior.BehavioralTimeSeries'>\n",
       "Fields:\n",
       "  time_series: { Speed d5 e1 <class 'pynwb.base.TimeSeries'>,  Speed d5 e2 <class 'pynwb.base.TimeSeries'>,  Speed d5 e3 <class 'pynwb.base.TimeSeries'>,  Speed d5 e4 <class 'pynwb.base.TimeSeries'>,  Speed d5 e5 <class 'pynwb.base.TimeSeries'>,  Speed d5 e6 <class 'pynwb.base.TimeSeries'>,  Speed d5 e7 <class 'pynwb.base.TimeSeries'>,  Speed d5 e8 <class 'pynwb.base.TimeSeries'>,  Speed d5 e9 <class 'pynwb.base.TimeSeries'> }"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Processing module for behavior\n",
    "behav_mod = nwbf.create_processing_module(name='Behavior', \n",
    "                                          description='Behavioral variables')\n",
    "# add the position, direction and speed data\n",
    "behav_mod.add_data_interface(pos)\n",
    "behav_mod.add_data_interface(dir)\n",
    "behav_mod.add_data_interface(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('franklab_apparatus',)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_path = \"franklab_apparatus.namespace.yaml\"\n",
    "pynwb.load_namespaces(ns_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep box\n",
    "Define the sleep box geometry as the minimum rectangle enclosing all of the animal positions across all sleep epochs in this day.\n",
    "Note that this is likely not the actual bounds of the sleep box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Sleep <class 'fl_apparatus.Task'>\n",
       "Fields:\n",
       "  description: The animal sleeps or wanders freely around a small, empty box."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sleep_pos = np.empty(shape=(0, 2))\n",
    "for epoch_num, task_epoch in task_struct.items():\n",
    "    if ('type' in task_epoch.keys()) and (task_epoch['type'][0] == 'sleep'):\n",
    "        sleep_pos = behav_mod.data_interfaces['Position']['Position d%d e%d' % (day, epoch_num)].data\n",
    "        all_sleep_pos = np.concatenate((all_sleep_pos, sleep_pos), axis=0)\n",
    "min_x, min_y = np.min(all_sleep_pos, axis=0)\n",
    "max_x, max_y = np.max(all_sleep_pos, axis=0)\n",
    "\n",
    "# Polygon vertices defined in clockwise order\n",
    "sleep_box_polygon = ap.PolygonNode(name='Sleep box polygon', \n",
    "                                   coords=[[min_x, min_y], [min_x, max_y], [max_x, max_y], [max_x, min_y]],\n",
    "                                   interior_coords=None)\n",
    "\n",
    "# The Sleep Box apparatus is just the sleep box polygon with no edges, since it is not connected to anything else\n",
    "sleep_box_appar = ap.Apparatus(name='Sleep Box', nodes=[sleep_box_polygon], edges=[])\n",
    "behav_mod.add_data_interface(sleep_box_appar)\n",
    "\n",
    "# Sleep task\n",
    "description = 'The animal sleeps or wanders freely around a small, empty box.'\n",
    "sleep_box_task = ap.Task(name='Sleep', description=description)\n",
    "behav_mod.add_data_interface(sleep_box_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------------------\n",
    "# # The start/stop time of epochs in NWB will be drawn from the start/stop times of data in pos_struct.\n",
    "# # (Start/stop times of epochs are not explicitly defined in the Matlab task_struct.)\n",
    "# # ------------------------------------------------------------------\n",
    "epoch_tags = ''  # tags is currently a required column in epochs\n",
    "\n",
    "# # Add metadata columns to the epochs table.\n",
    "# # Note: in Bon data, epochs of type 'sleep' do not contain environment and exposure metadata\n",
    "# nwbf.add_epoch_column(name='epoch_type', description='type of the epoch (i.e. run, sleep)')\n",
    "# nwbf.add_epoch_column(name='environment', description='behavioral environment of the epoch (i.e. W-Track A)')\n",
    "nwbf.add_epoch_column(name='exposure', description='number of exposures to this environment')\n",
    "nwbf.add_epoch_column(name='task', description='behavioral task for this epoch')\n",
    "nwbf.add_epoch_column(name='apparatus', description='behavioral apparatus for this epoch')\n",
    "\n",
    "all_sleep_pos = np.empty(shape=(0, 2))\n",
    "for epoch_num, task_epoch in task_struct.items():\n",
    "    # Required columns\n",
    "    epoch_start, epoch_stop = pos_time_ivls[epoch_num-1]  # start/stop times inferred from pos_struct\n",
    "    m_per_pixel = pos_struct[epoch_num]['cmperpixel'][0,0]/100  # convert pixels to meters\n",
    "    \n",
    "    # Metadata (some not present in 'sleep' epochs)\n",
    "    # Use these to get the task and apparatus below\n",
    "    epoch_metadata_keys = task_epoch.keys()\n",
    "    if 'type' in epoch_metadata_keys:\n",
    "        epoch_type = task_epoch['type'][0]\n",
    "    else:\n",
    "        epoch_type = 'NA'\n",
    "    if 'environment' in epoch_metadata_keys:\n",
    "        epoch_env = task_epoch['environment'][0]\n",
    "    else:\n",
    "        epoch_env = 'NA'\n",
    "    if 'exposure' in epoch_metadata_keys:\n",
    "        epoch_exposure = task_epoch['exposure'][0]\n",
    "    else:\n",
    "        epoch_exposure = 'NA'\n",
    "        \n",
    "    # Get the Task for this epoch.\n",
    "    if epoch_type == 'sleep':\n",
    "        appar = behav_mod.data_interfaces['Sleep Box']\n",
    "        task = behav_mod.data_interfaces[\"Sleep\"]\n",
    "    elif epoch_type == 'run':\n",
    "        task_name = 'W Alternation'\n",
    "        if task_name not in behav_mod.data_interfaces.keys():   # Create a new Task if necessary\n",
    "            task_description = 'The animal runs in an alternating W pattern between neighboring arms of a maze.'\n",
    "            behav_mod.add_data_interface(ap.Task(name=task_name, description=task_description))\n",
    "        task = behav_mod.data_interfaces[task_name]\n",
    "    else:  # epoch type not 'sleep' or 'run'\n",
    "        if epoch_type not in behav_mod.data_interfaces.keys():\n",
    "            task_description = \"Epoch task: {}\".format(epoch_type)\n",
    "            behav_mod.add_data_interface(ap.Task(name=epoch_type, description=task_description))\n",
    "        task = behav_mod.data_interfaces[task_name]\n",
    "        \n",
    "    # Get the Apparatus for this epoch\n",
    "    if epoch_type == 'sleep':\n",
    "        appar = behav_mod.data_interfaces['Sleep Box']\n",
    "    else:\n",
    "        if epoch_env not in behav_mod.data_interfaces.keys():\n",
    "            appar = ap.get_apparatus_from_linpos(linpos_struct[epoch_num], epoch_env, conversion=m_per_pixel) \n",
    "            behav_mod.add_data_interface(appar)\n",
    "        appar = behav_mod.data_interfaces[epoch_env]\n",
    "            \n",
    "    \n",
    "    # Add this epoch to the NWB file\n",
    "    # We include soft link to this task, which itself contains a link to the apparatus\n",
    "    nwbf.add_epoch(start_time=epoch_start,\n",
    "                   stop_time=epoch_stop,\n",
    "                   exposure=epoch_exposure,\n",
    "                   task=task,\n",
    "                   apparatus=appar,\n",
    "                   tags=epoch_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetrode info\n",
    "Load in `tetinfo` struct and populate ElectrodeTable, electrode groups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the electrode table.\n",
    "# The logic here is as follows:\n",
    "#   Each Tetrode gets its own ElectrodeGroup and ElectrodeTableRegion\n",
    "#   Each individual recording channel gets its own row in nwbfile.electrodes\n",
    "\n",
    "# we first create the ElectrodeTable that all the electrodes will go into\n",
    "nchan_per_tetrode = 4 #these files all contain tetrodes, so we assume four channels\n",
    "tetinfo_filename = \"%s/%s%s\" % (data_dir, prefix, tetinfo_file)\n",
    "recording_device = nwbf.create_device(name='NSpike acquisition system')\n",
    "tet_electrode_group = {}\n",
    "tet_electrode_table_region = {}\n",
    "lfp_electrode_table_region = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Using tetrode numbers:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "mat = ns.loadmat_ff(tetinfo_filename, 'tetinfo')\n",
    "#only look at first epoch because rest are duplicates\n",
    "tets = mat[day][1]\n",
    "\n",
    "# For debugging, limit number of tets to import\n",
    "subset_keys = sorted(tets.keys())[0:limit_num_of_tets]\n",
    "tets = {k:v for (k,v) in tets.items() if k in subset_keys}\n",
    "\n",
    "print(limit_num_of_tets)\n",
    "print(\"Using tetrode numbers:\")\n",
    "print(subset_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tets[1]['depth'][0,0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenny's data has a nested [day][epoch][tetrode] structure but duplicates the info across epochs, so we can just\n",
    "# use the first epoch for everything\n",
    "chan_num = 0 # this will hold an incrementing channel number for the entire day of data\n",
    "for tet_num, tet in tets.items():\n",
    "    #print('making electrode group for day %d, tet %d' % (day, tet_ind))\n",
    "    # go through the list of fields\n",
    "    hemisphere = '?'\n",
    "    # tet.area/.subarea are 1-d arrays of Unicode strings\n",
    "    area = str(tet['area'][0]) if 'area' in tet else '?' # h5py barfs on numpy.str_ type objects?\n",
    "    if 'sub_area' in tet: \n",
    "        sub_area = str(tet['sub_area'][0]) # h5py barfs on numpy.str_ type objects?\n",
    "        location = area + ' ' + sub_area\n",
    "    else:\n",
    "        sub_area = '?'\n",
    "        location = area \n",
    "\n",
    "    # tet.depth is a 1x1 cell array in tetinfo struct for some reason (multiple depths?)\n",
    "    # (which contains the expected 1x1 numeric array)\n",
    "    coord = [np.nan, np.nan, tet['depth'][0, 0][0, 0] / 12 / 80 * 25.4] if 'depth' in tet else [np.nan, np.nan, np.nan]\n",
    "    impedance = np.nan\n",
    "    filtering = 'unknown - likely 600Hz-6KHz'\n",
    "\n",
    "    channel_location = [location, location, location, location]\n",
    "    channel_coordinates = [coord, coord, coord, coord]\n",
    "    electrode_name = \"%02d-%02d\" % (day, tet_num)\n",
    "    description = \"tetrode {tet_num} located in {location} on day {day}\".format(tet_num=tet_num,\n",
    "                                                                               location=location,\n",
    "                                                                               day=day)\n",
    "\n",
    "    # we need to create an electrode group for this tetrode\n",
    "    tet_electrode_group[tet_num] = nwbf.create_electrode_group(name=electrode_name,\n",
    "                                                        description=description,\n",
    "                                                        location=location,\n",
    "                                                        device=recording_device)\n",
    "\n",
    "    for i in range(nchan_per_tetrode):\n",
    "            # now add an electrode\n",
    "            nwbf.add_electrode(x=coord[0],\n",
    "                               y=coord[1],\n",
    "                               z=coord[2],\n",
    "                               imp=impedance,\n",
    "                               location=location,\n",
    "                               filtering=filtering,\n",
    "                               group=tet_electrode_group[tet_num],\n",
    "                               group_name=tet_electrode_group[tet_num].name, # not in docstring??\n",
    "                               id=chan_num)\n",
    "            chan_num = chan_num + 1\n",
    "\n",
    "    # now that we've created four entries, one for each channel of the tetrode, we create a new\n",
    "    # electrode table region for this tetrode and number it appropriately\n",
    "    table_region_description = 'ntrode %d region' % tet_num\n",
    "    table_region_name = '%d' % tet_num\n",
    "    tet_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=list(range(chan_num-nchan_per_tetrode,chan_num)),\n",
    "        description=table_region_description,\n",
    "        # BUG #679: name must be 'electrodes' or NWB file will not be readable\n",
    "        name='electrodes') #        name=table_region_name)\n",
    "\n",
    "\n",
    "\n",
    "    # Also create electrode_table_regions for each tetrode's LFP recordings\n",
    "    # (Assume that LFP is taken from the first channel)\n",
    "    lfp_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=[chan_num-nchan_per_tetrode],\n",
    "        description=table_region_description,\n",
    "        # BUG #679: name must be 'electrodes' or NWB file will not be readable\n",
    "        name='electrodes') #        name=table_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tet_electrode_table_region[1].region\n",
    "# nwbf.ec_electrode_groups['03-01'].description\n",
    "# tet_electrode_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing LFP data for day  5\n",
      " -> tet_num: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eeg_files = ns.get_eeg_by_day(eeg_path, prefix, 'eeg')\n",
    "lfp_data = []\n",
    "\n",
    "lfp = pynwb.ecephys.LFP(electrical_series=lfp_data)\n",
    "# read data from EEG/*eeg*.mat files and build TimeSeries object\n",
    "\n",
    "print('processing LFP data for day %2d' % day)\n",
    "for tet_num in tets.keys():\n",
    "    print(' -> tet_num: %d' % tet_num)\n",
    "    timestamps, data = ns.build_day_eeg(eeg_files[day][tet_num], eeg_samprate)\n",
    "    timestamps += NSpike_posixtime_offset\n",
    "    name = \"{prefix}eeg-{day}-{tet}\".format(prefix=prefix, day=day, tet=tet_num)\n",
    "    lfp.create_electrical_series(name=name, \n",
    "                                 data=data / 1000, # convert mV to V, as expected\n",
    "                                 electrodes=lfp_electrode_table_region[tet_num],\n",
    "                                 timestamps=timestamps)\n",
    "nwbf.add_acquisition(lfp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unit metadata first\n",
    "# External clustering software gives names for each cluster--we want to preserve these\n",
    "nwbf.add_unit_column('cluster_name',  '(str) cluster name from clustering software')\n",
    "\n",
    "# # For tetrode data, this will usually be all channels in the tetrode (2018Dec03--now in spec with 'electrodes' field?)\n",
    "# nwbf.add_unit_column('neighborhood',  '(electrodeTableRegion) list of electrodes on which spikes were clustered')\n",
    "\n",
    "# # AKA 'Valid_times'--the times during which a spike from this cluster could have possibly been observed.\n",
    "# # (handle periods between behavior epochs, acquisition system dropouts, etc.)\n",
    "# nwbf.add_unit_column('obs_intervals', '(intervalSeries) Observation Intervals for the spike times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading spikes file :/Users/ericmiller/Data/FrankData/kkay/Bon/bonspikes05.mat\n"
     ]
    }
   ],
   "source": [
    "#get the spike times from the spikes files\n",
    "#each cluster gets a unique number starting at zero\n",
    "\n",
    "spike_files = ns.get_files_by_day(data_dir, prefix, 'spikes')\n",
    "print('\\nLoading spikes file :' + spike_files[day])\n",
    "mat = ns.loadmat_ff(spike_files[day], 'spikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_unit = []\n",
    "obs_intervals = {}\n",
    "cluster_by_tet = {}\n",
    "cluster_id = 0\n",
    "\n",
    "# Matlab structs are nested by: day, epoch, tetrode, cluster, but we will want to save all spikes from a give cluster\n",
    "# *across multiple epochs* in same spike list. So we rearrange the nested matlab structures for convenience. We \n",
    "# create a nested dict, keyed by 1) tetrode, 2) cluster number, then 3) epoch. NB the keys are 1-indexed, to be \n",
    "# consistent with the original data collection. (We only process one day at a time for now, so no need to nest days).\n",
    "\n",
    "spike_struct = mat[day]\n",
    "for epoch_num, espikes in spike_struct.items():\n",
    "    for tet_num, tspikes in espikes.items():\n",
    "        # respect tet subset selection done above\n",
    "        if tet_num not in tets.keys():\n",
    "            continue\n",
    "        if tet_num not in cluster_by_tet.keys():\n",
    "            cluster_by_tet[tet_num] = {}\n",
    "        for cluster_num, cspikes in tspikes.items():\n",
    "            if cluster_num not in cluster_by_tet[tet_num].keys():\n",
    "                cluster_by_tet[tet_num][cluster_num] = {}\n",
    "            cluster_by_tet[tet_num][cluster_num][epoch_num] = cspikes\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cluster id:   0, name: d5 t1 c1\n",
      "Adding cluster id:   1, name: d5 t1 c2\n",
      "Adding cluster id:   2, name: d5 t1 c3\n",
      "Adding cluster id:   3, name: d5 t1 c4\n",
      "Adding cluster id:   4, name: d5 t1 c5\n",
      "Adding cluster id:   5, name: d5 t1 c6\n",
      "Adding cluster id:   6, name: d5 t1 c7\n",
      "Adding cluster id:   7, name: d5 t1 c8\n",
      "Adding cluster id:   8, name: d5 t1 c9\n",
      "Adding cluster id:   9, name: d5 t2 c1\n",
      "Adding cluster id:  10, name: d5 t2 c2\n",
      "Adding cluster id:  11, name: d5 t2 c3\n",
      "Adding cluster id:  12, name: d5 t2 c4\n",
      "Adding cluster id:  13, name: d5 t2 c5\n",
      "Adding cluster id:  14, name: d5 t3 c1\n",
      "Adding cluster id:  15, name: d5 t3 c2\n",
      "Adding cluster id:  16, name: d5 t5 c1\n",
      "Adding cluster id:  17, name: d5 t5 c2\n",
      "Adding cluster id:  18, name: d5 t5 c3\n",
      "Adding cluster id:  19, name: d5 t5 c4\n",
      "Adding cluster id:  20, name: d5 t8 c1\n",
      "Adding cluster id:  21, name: d5 t8 c2\n",
      "Adding cluster id:  22, name: d5 t10 c1\n",
      "Adding cluster id:  23, name: d5 t11 c1\n",
      "Adding cluster id:  24, name: d5 t11 c2\n",
      "Adding cluster id:  25, name: d5 t11 c3\n",
      "Adding cluster id:  26, name: d5 t11 c4\n",
      "Adding cluster id:  27, name: d5 t13 c1\n",
      "Adding cluster id:  28, name: d5 t13 c2\n",
      "Adding cluster id:  29, name: d5 t14 c1\n",
      "Adding cluster id:  30, name: d5 t14 c2\n",
      "Adding cluster id:  31, name: d5 t14 c3\n",
      "Adding cluster id:  32, name: d5 t14 c4\n",
      "Adding cluster id:  33, name: d5 t14 c5\n",
      "Adding cluster id:  34, name: d5 t17 c1\n",
      "Adding cluster id:  35, name: d5 t18 c1\n",
      "Adding cluster id:  36, name: d5 t18 c2\n",
      "Adding cluster id:  37, name: d5 t18 c3\n",
      "Adding cluster id:  38, name: d5 t18 c4\n",
      "Adding cluster id:  39, name: d5 t18 c5\n",
      "Adding cluster id:  40, name: d5 t19 c1\n",
      "Adding cluster id:  41, name: d5 t19 c2\n",
      "Adding cluster id:  42, name: d5 t19 c3\n",
      "Adding cluster id:  43, name: d5 t20 c1\n",
      "Adding cluster id:  44, name: d5 t20 c2\n",
      "Adding cluster id:  45, name: d5 t20 c3\n",
      "Adding cluster id:  46, name: d5 t20 c4\n",
      "Adding cluster id:  47, name: d5 t20 c5\n",
      "Adding cluster id:  48, name: d5 t22 c1\n",
      "Adding cluster id:  49, name: d5 t22 c2\n",
      "Adding cluster id:  50, name: d5 t23 c1\n",
      "Adding cluster id:  51, name: d5 t23 c2\n",
      "Adding cluster id:  52, name: d5 t23 c3\n",
      "Adding cluster id:  53, name: d5 t23 c4\n",
      "Adding cluster id:  54, name: d5 t23 c5\n",
      "Adding cluster id:  55, name: d5 t25 c1\n",
      "Adding cluster id:  56, name: d5 t25 c2\n",
      "Adding cluster id:  57, name: d5 t25 c3\n",
      "Adding cluster id:  58, name: d5 t28 c1\n",
      "Adding cluster id:  59, name: d5 t28 c2\n"
     ]
    }
   ],
   "source": [
    "# now we create the SpikeEventStructures and their containing EventWaveform objects\n",
    "colidx_timestamps = 0\n",
    "\n",
    "for tet_num in cluster_by_tet.keys():\n",
    "    obs_intervals[tet_num] = {}\n",
    "    for cluster_num in cluster_by_tet[tet_num].keys():\n",
    "        cluster_name = 'd%d t%d c%d' % (day, tet_num, cluster_num)\n",
    "        print('Adding cluster id: %3d, name: %s' % (cluster_id, cluster_name))\n",
    "\n",
    "        cluster_tmp = cluster_by_tet[tet_num][cluster_num]\n",
    "\n",
    "#         # construct a full data array and a parallel list of observation intervals\n",
    "#         obs_intervals[tet_num][cluster_num] = pynwb.misc.IntervalSeries(\n",
    "#                                                     name = cluster_name,\n",
    "#                                                     description = 'Observation intervals for spikes from cluster ' +\n",
    "#                                                     str(cluster_num) + ' on tetrode ' + str(tet_num))\n",
    "        obs_intervals[tet_num][cluster_num] = np.zeros([0,2])\n",
    "\n",
    "        spikes_ep = []\n",
    "        for epoch in cluster_tmp.keys():\n",
    "            if cluster_tmp[epoch]['data'].shape[0]:\n",
    "                spikes_ep.append(cluster_tmp[epoch]['data'][:,colidx_timestamps] + NSpike_posixtime_offset)\n",
    "            for obs_intervals_cl_ep in cluster_tmp[epoch]['timerange']:\n",
    "                # 'timerange' for each cell is given in NSpike timestamp units\n",
    "                obs_int = (obs_intervals_cl_ep.T.astype(float)/NSpike_timestamps_per_sec) + NSpike_posixtime_offset\n",
    "                obs_intervals[tet_num][cluster_num] = np.append(obs_intervals[tet_num][cluster_num], [obs_int], axis=0)\n",
    "                #                obs_intervals[tet_num][cluster_num].add_interval(*obs_int)\n",
    "\n",
    "        spiketimes = np.concatenate(spikes_ep)\n",
    "\n",
    "        # Add Observation Intervals to nwbfile willy-nilly (1 per cluster), \n",
    "        # so that we can successfully refer to them in the Unit metadata table\n",
    "#        spike_mod.add_data_interface(obs_intervals[tet_num][cluster_num])\n",
    "\n",
    "#         nwbf.add_unit(data = {'cluster_name': cluster_name, \n",
    "#                               'elec_group': tet_electrode_group[tet_num], \n",
    "#                               # can't just refer to electrode_table_region itself?: are never added to nwbfile to \n",
    "#                               # begin with. Instead, use 'data' field, which is a list of electrodeTable indices.\n",
    "# #                               'neighborhood': tet_electrode_table_region[tet_num], # tet_electrode_table_region[tet_num].data, \n",
    "#                               'obs_intervals': obs_intervals[tet_num][cluster_num]},\n",
    "#                       id = cluster_id)\n",
    "\n",
    "#         spike_UnitTimes.add_spike_times(cluster_id, spiketimes)\n",
    "\n",
    "        nwbf.add_unit(spike_times= spiketimes,\n",
    "                      electrodes=tet_electrode_table_region[tet_num].data, # wants list of electrode ids, not an electrodeTableRegion\n",
    "#                       electrode_group=[tet_electrode_group[tet_num]], # we can't figure out what is an acceptable input, here:\n",
    "#                                                                       # TypeError: incorrect type for 'container' (got 'list', expected 'Builder, Container or ReferenceBuilder')\n",
    "                      cluster_name=cluster_name, \n",
    "                      obs_intervals=obs_intervals[tet_num][cluster_num],\n",
    "                      id = cluster_id)\n",
    "\n",
    "        cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out NWBfile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote nwb file: Bon05_test.nwb\n"
     ]
    }
   ],
   "source": [
    "# make an NWBFile\n",
    "with pynwb.NWBHDF5IO(nwb_filename, mode='w') as iow:\n",
    "    iow.write(nwbf)\n",
    "print('Wrote nwb file: ' + nwb_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]\n",
      "(\n",
      "cluster_name <class 'pynwb.core.VectorData'>\n",
      "Fields:\n",
      "  description: (str) cluster name from clustering software\n",
      ", \n",
      "spike_times_index <class 'pynwb.core.VectorIndex'>\n",
      "Fields:\n",
      "  target: spike_times <class 'pynwb.core.VectorData'>\n",
      ", \n",
      "spike_times <class 'pynwb.core.VectorData'>\n",
      "Fields:\n",
      "  description: the spike times for each unit\n",
      ", \n",
      "obs_intervals_index <class 'pynwb.core.VectorIndex'>\n",
      "Fields:\n",
      "  target: obs_intervals <class 'pynwb.core.VectorData'>\n",
      ", \n",
      "obs_intervals <class 'pynwb.core.VectorData'>\n",
      "Fields:\n",
      "  description: the observation intervals for each unit\n",
      ", \n",
      "electrodes_index <class 'pynwb.core.VectorIndex'>\n",
      "Fields:\n",
      "  target: electrodes <class 'pynwb.core.DynamicTableRegion'>\n",
      ", \n",
      "electrodes <class 'pynwb.core.DynamicTableRegion'>\n",
      "Fields:\n",
      "  description: the electrodes that each spike unit came from\n",
      "  table: electrodes <class 'pynwb.core.DynamicTable'>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(nwbf.electrodes.id.data)\n",
    "print(nwbf.units.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our NWBfile, and check some roundtrip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf_read = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_id = 1\n",
    "print(nwbf_read.units.get_unit_spike_times(cl_id).shape)\n",
    "print(nwbf_read.units['cluster_name'][cl_id])\n",
    "print(nwbf_read.units['obs_intervals'][cl_id])\n",
    "print(nwbf_read.units.get_unit_obs_intervals(cl_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(nwbf.epochs['epoch_type'][:]) == 'run')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the electrical_series objects and work with it.\n",
    "example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[-1]\n",
    "nwbf_read.get_acquisition('LFP').electrical_series\n",
    "\n",
    "def fmt_sec_from_midnight (x, pos):\n",
    "    x_posix = mdates.num2epoch(x)\n",
    "    x_dt = mdates.num2date(x) \n",
    "    prev_midnight = x_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return (x_dt - prev_midnight).total_seconds()\n",
    "#    return \"%0.6f\" % mdates.num2epoch(x)\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,10))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "print(ax1.xaxis.major.formatter)\n",
    "xtick_locator = mticker.AutoLocator()\n",
    "xtick_formatter = mticker.FuncFormatter(qu.fmt_truncate_posix)\n",
    "# xtick_formatter = mdates.AutoDateFormatter(xtick_locator)\n",
    "# xtick_formatter = mticker.ScalarFormatter(xtick_locator)\n",
    "\n",
    "\n",
    "for i in [1]: #range(len(list(nwbf_read.get_acquisition('LFP').electrical_series.items()))):\n",
    "    example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[i]\n",
    "    nwbf_read.get_acquisition('LFP').electrical_series\n",
    "    #ts = (example_es.timestamps[-1001:-1]*1e9).astype('datetime64[ns]')\n",
    "    ts = example_es.timestamps[-10001:-1] \n",
    "    print(\"plot #%d\" % i)\n",
    "    ax1.plot(ts, example_es.data[0:10000] * 1000)\n",
    "\n",
    "ax1.set_title(example_es_name)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "# ax1.set_xlabel('Time (s; prefix = 113615\\u2026)')\n",
    "ax1.set_ylabel('Amplitude (mV)')\n",
    "ax1.xaxis.set_major_locator(xtick_locator)\n",
    "ax1.xaxis.set_major_formatter(xtick_formatter)\n",
    "print(ax1.xaxis.major.formatter)\n",
    "\n",
    "\n",
    "# xtick_locator = mdates.AutoDateLocator()\n",
    "#fig1.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

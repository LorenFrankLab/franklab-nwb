{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "\n",
    "import pynwb\n",
    "import nspike_helpers as ns \n",
    "import query_helpers as qu\n",
    "\n",
    "mdates.rcParams.update({'date.autoformatter.microsecond': '%H:%M:%S.%f'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug settings\n",
    "limit_num_of_tets = None # To speed up testing. Set to None to load all tets\n",
    "\n",
    "# Session-specific params\n",
    "# data_dir = '/opt/data46/FrankData/kkay/Bon/'\n",
    "data_dir = os.path.expanduser('~/Data/FrankData/kkay/Bon')\n",
    "anim = 'Bon' \n",
    "day = 4 # below we'll code date as 2006-Jan-'Day'\n",
    "\n",
    "# 'Wall clock' (i.e. actual) date and time of the Nspike time = 0 for this experiment.\n",
    "# NOTE: this is not the true zero_time, as we don't have easy access to that without digging in lab notebooks.\n",
    "# TODO: dig in lab notebooks, at least to get the date of recording.\n",
    "dataset_zero_time = datetime(2006, 1, day, 12, 0, 0, tzinfo=tz.gettz('US/Pacific'))\n",
    "\n",
    "# General params/presets\n",
    "file_create_date = datetime.now(tz.tzlocal())\n",
    "\n",
    "eeg_samprate = 1500.0 # Hz\n",
    "\n",
    "eeg_subdir = \"EEG\"\n",
    "epochs_file = \"times.mat\"\n",
    "tetinfo_file = \"tetinfo.mat\"\n",
    "NSpike_timestamps_per_sec = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_create_date)\n",
    "print(dataset_zero_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse inputs and create NWBfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_str = '%02d' % day\n",
    "\n",
    "nwb_filename = anim + day_str + '_test.nwb'\n",
    "\n",
    "# check the input arguments\n",
    "if not os.path.exists(data_dir):\n",
    "        print('Error: data_dir %s does not exist' % data_dir)\n",
    "        exit(-1)\n",
    "\n",
    "# get filename prefix and file locations\n",
    "prefix = anim.lower()\n",
    "eeg_path = os.path.join(data_dir, eeg_subdir)\n",
    "\n",
    "# Calculate the POSIX timestamp when Nspike clock = 0 (seconds)\n",
    "NSpike_posixtime_offset = dataset_zero_time.timestamp()\n",
    "\n",
    "# We'll still store NSpike/Trodes zero time in nwbfile.session_start_time, \n",
    "# so that we can recreate the experimental timestamps, even though that\n",
    "# violates the definition of session_start_time as zero time. We need some\n",
    "# other way to indicate that the timestamps are POSIX (easy: if they are over\n",
    "# a trillion, then they're POSIX!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf = pynwb.NWBFile(\n",
    "           session_description='Converted NSpike data from %s' % data_dir,\n",
    "           identifier=anim+day_str,\n",
    "           session_start_time=dataset_zero_time,\n",
    "           file_create_date=file_create_date,\n",
    "           lab='Frank Laboratory',\n",
    "           experimenter='Mattias Karlsson',\n",
    "           institution='UCSF',\n",
    "           experiment_description='Recordings from awake behaving rat',\n",
    "           session_id=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create position, direction and speed\n",
    "position_list = []\n",
    "pos = pynwb.behavior.Position(spatial_series=position_list, \n",
    "                              name='Position')\n",
    "\n",
    "direction_list = []\n",
    "dir = pynwb.behavior.CompassDirection(spatial_series=direction_list, \n",
    "                                      name='Head Direction')\n",
    "\n",
    "speed_list = []\n",
    "speed = pynwb.behavior.BehavioralTimeSeries(time_series=speed_list, \n",
    "                                            name='Speed')\n",
    "\n",
    "# NOTE that day_inds is 0 based\n",
    "time_list = {}\n",
    "nwb_epoch = {}\n",
    "pos_files = ns.get_files_by_day(data_dir, prefix, 'pos')\n",
    "task_files = ns.get_files_by_day(data_dir, prefix, 'task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat_ff(task_files[day], 'task')\n",
    "task_struct = mat[day]\n",
    "# find the pos file for this day and load it\n",
    "\n",
    "mat = ns.loadmat_ff(pos_files[day], 'pos')\n",
    "pos_struct = mat[day]\n",
    "\n",
    "# compile a list of time intervals in an array and create the position, head direction and velocity structures\n",
    "time_list = []\n",
    "\n",
    "# Assume field order: (time,x,y,dir,vel)\n",
    "(time_idx, x_idx, y_idx, dir_idx, vel_idx) = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num, pos_epoch in pos_struct.items():\n",
    "\n",
    "    # convert times to POSIX time\n",
    "    timestamps = pos_epoch['data'][:,time_idx] + NSpike_posixtime_offset\n",
    "\n",
    "    # TODO: create a shared TimeSeries for timestamps, across all behavioral timeseries\n",
    "    # ?? timestamps_obj = pynwb.TimeSeries(timestamps=timestamps...)\n",
    "\n",
    "    # collect times of epoch start and end\n",
    "    time_list.append([timestamps[0], timestamps[-1]])\n",
    "\n",
    "    m_per_pixel = pos_epoch['cmperpixel'][0,0]/100 # NWB wants meters per pixel\n",
    "\n",
    "    # we can also create new SpatialSeries for the position, direction and velocity information\n",
    "    #NOTE: Each new spatial series has to have a unique name.\n",
    "    pos.create_spatial_series(name='Position d%d e%d' % (day, epoch_num), \n",
    "                              timestamps = timestamps,\n",
    "                              data=pos_epoch['data'][:, (x_idx, y_idx)] * m_per_pixel,\n",
    "                              reference_frame='corner of video frame',\n",
    "                              #conversion=m_per_pixel,\n",
    "                              #unit='m'\n",
    "                              ) # *after* conversion\n",
    "\n",
    "    dir.create_spatial_series(name='Head Direction d%d e%d'% (day, epoch_num), \n",
    "                              timestamps=timestamps,\n",
    "                              data=pos_epoch['data'][:, dir_idx],\n",
    "                              reference_frame='0=facing top of video frame (?), positive clockwise (?)',\n",
    "                              #unit='radians'\n",
    "                              )\n",
    "\n",
    "    speed.create_timeseries(name='Speed d%d e%d' % (day, epoch_num),\n",
    "                             timestamps=timestamps,\n",
    "                             data=pos_epoch['data'][:, vel_idx] * m_per_pixel,\n",
    "                             unit='m/s',\n",
    "                             #conversion=m_per_pixel,\n",
    "                             description='smoothed movement speed estimate')\n",
    "time_list = np.asarray(time_list)\n",
    "                                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Processing module for behavior\n",
    "behav_mod = nwbf.create_processing_module(name='Behavior', \n",
    "                                          description='Behavioral variables')\n",
    "# add the position, direction and speed data\n",
    "behav_mod.add_data_interface(pos)\n",
    "behav_mod.add_data_interface(dir)\n",
    "behav_mod.add_data_interface(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs (Not currently implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list to store all of the fields in the task structure and \n",
    "# # a parallel list to store the created task interval structures\n",
    "# task_fields = []\n",
    "# task_intervals = []\n",
    "\n",
    "# # each day will be defined as a single Epoch in NWB so we go through and get the first and last time from the\n",
    "# # position data\n",
    "# day_start = time_list[0,0]\n",
    "# day_end = time_list[-1,1]\n",
    "\n",
    "# nwb_epoch = nwbf.create_epoch('day %s' % day, data_source, day_start, day_end, [], 'day %s' % day)\n",
    "# # add ignore intervals for the spaces between our epochs (it's not clear if this is necessary, but it won't hurt)\n",
    "# # also, there's probably a more \"python-ic\" way to do this, but I don't know what it is 8-)\n",
    "# if len(time_list) > 1:\n",
    "#         n = 1\n",
    "#         while n <= #NO!# len(time_list):\n",
    "#                 #nwb_epoch.add_ignore_interval(time_list[n-1][1], time_list[n][0])\n",
    "#                 n += 1\n",
    "\n",
    "# # now we go through the task structure and add a new interval series or an interval to an existing interval series\n",
    "# # for each element in the task structure\n",
    "# for epoch_num, task_epoch in enumerate(task_struct):\n",
    "#         if task_epoch.size > 0:\n",
    "#                 task_epoch = task_epoch[0,0] # retrieve dict from 1x1 ndarray\n",
    "#                 for field_name, value in task_epoch.items()\n",
    "#                         if field_name not in task_fields:\n",
    "#                                 # add the field_name to the list and create a new IntervalSeries for it\n",
    "#                                 task_fields.append(field_name)\n",
    "# #                               tmp_array = np.ndarray(2);\n",
    "# #                               tmp_array = [time_list[epoch_num][0], time_list[epoch_num][1]]\n",
    "#                                 tmp_interval = IntervalSeries(field_name, 'matlab task structure')\n",
    "#                                 # add the interval for this epoch\n",
    "#                                 tmp_interval.add_interval(*time_list[epoch_ind,:])\n",
    "#                                 task_intervals.append(tmp_interval)\n",
    "#                         else:\n",
    "#                                 # add the interval to appropriate element of the list\n",
    "#                                 task_intervals[task_fields.index(field_name)].add_interval(*time_list[epoch_ind])\n",
    "\n",
    "\n",
    "# # Now add the complete list of task intervals to the behav_mod module\n",
    "# for interval in task_intervals:\n",
    "#         behav_mod.add_data_interface(BehavioralEpochs('task information', interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetrode info\n",
    "Load in `tetinfo` struct and populate ElectrodeTable, electrode groups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the electrode table.\n",
    "# The logic here is as follows:\n",
    "#   Each Tetrode gets its own ElectrodeGroup and ElectrodeTableRegion\n",
    "#   Each individual recording channel gets its own row in nwbfile.electrodes\n",
    "\n",
    "# we first create the ElectrodeTable that all the electrodes will go into\n",
    "nchan_per_tetrode = 4 #these files all contain tetrodes, so we assume four channels\n",
    "tetinfo_filename = \"%s/%s%s\" % (data_dir, prefix, tetinfo_file)\n",
    "recording_device = nwbf.create_device(name='NSpike acquisition system')\n",
    "tet_electrode_group = {}\n",
    "tet_electrode_table_region = {}\n",
    "lfp_electrode_table_region = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat_ff(tetinfo_filename, 'tetinfo')\n",
    "#only look at first epoch because rest are duplicates\n",
    "tets = mat[day][1]\n",
    "\n",
    "# For debugging, limit number of tets to import\n",
    "subset_keys = sorted(tets.keys())[0:limit_num_of_tets]\n",
    "tets = {k:v for (k,v) in tets.items() if k in subset_keys}\n",
    "\n",
    "print(limit_num_of_tets)\n",
    "print(\"Using tetrode numbers:\")\n",
    "print(subset_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tets[1]['depth'][0,0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenny's data has a nested [day][epoch][tetrode] structure but duplicates the info across epochs, so we can just\n",
    "# use the first epoch for everything\n",
    "chan_num = 0 # this will hold an incrementing channel number for the entire day of data\n",
    "for tet_num, tet in tets.items():\n",
    "    #print('making electrode group for day %d, tet %d' % (day, tet_ind))\n",
    "    # go through the list of fields\n",
    "    hemisphere = '?'\n",
    "    # tet.area/.subarea are 1-d arrays of Unicode strings\n",
    "    area = str(tet['area'][0]) if 'area' in tet else '?' # h5py barfs on numpy.str_ type objects?\n",
    "    if 'sub_area' in tet: \n",
    "        sub_area = str(tet['sub_area'][0]) # h5py barfs on numpy.str_ type objects?\n",
    "        location = area + ' ' + sub_area\n",
    "    else:\n",
    "        sub_area = '?'\n",
    "        location = area \n",
    "\n",
    "    # tet.depth is a 1x1 cell array in tetinfo struct for some reason (multiple depths?)\n",
    "    # (which contains the expected 1x1 numeric array)\n",
    "    coord = [np.nan, np.nan, tet['depth'][0, 0][0, 0] / 12 / 80 * 25.4] if 'depth' in tet else [np.nan, np.nan, np.nan]\n",
    "    impedance = np.nan\n",
    "    filtering = 'unknown - likely 600Hz-6KHz'\n",
    "\n",
    "    channel_location = [location, location, location, location]\n",
    "    channel_coordinates = [coord, coord, coord, coord]\n",
    "    electrode_name = \"%02d-%02d\" % (day, tet_num)\n",
    "    description = \"tetrode {tet_num} located in {location} on day {day}\".format(tet_num=tet_num,\n",
    "                                                                               location=location,\n",
    "                                                                               day=day)\n",
    "\n",
    "    # we need to create an electrode group for this tetrode\n",
    "    tet_electrode_group[tet_num] = nwbf.create_electrode_group(name=electrode_name,\n",
    "                                                        description=description,\n",
    "                                                        location=location,\n",
    "                                                        device=recording_device)\n",
    "\n",
    "    for i in range(nchan_per_tetrode):\n",
    "            # now add an electrode\n",
    "            nwbf.add_electrode(x=coord[0],\n",
    "                               y=coord[1],\n",
    "                               z=coord[2],\n",
    "                               imp=impedance,\n",
    "                               location=location,\n",
    "                               filtering=filtering,\n",
    "                               group=tet_electrode_group[tet_num],\n",
    "                               group_name=tet_electrode_group[tet_num].name, # not in docstring??\n",
    "                               id=chan_num)\n",
    "            chan_num = chan_num + 1\n",
    "\n",
    "    # now that we've created four entries, one for each channel of the tetrode, we create a new\n",
    "    # electrode table region for this tetrode and number it appropriately\n",
    "    table_region_description = 'ntrode %d region' % tet_num\n",
    "    table_region_name = '%d' % tet_num\n",
    "    tet_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=list(range(chan_num-nchan_per_tetrode,chan_num)),\n",
    "        description=table_region_description,\n",
    "        # BUG #679: name must be 'electrodes' or NWB file will not be readable\n",
    "        name='electrodes') #        name=table_region_name)\n",
    "\n",
    "\n",
    "\n",
    "    # Also create electrode_table_regions for each tetrode's LFP recordings\n",
    "    # (Assume that LFP is taken from the first channel)\n",
    "    lfp_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=[chan_num-nchan_per_tetrode],\n",
    "        description=table_region_description,\n",
    "        # BUG #679: name must be 'electrodes' or NWB file will not be readable\n",
    "        name='electrodes') #        name=table_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tet_electrode_table_region[1].region\n",
    "# nwbf.ec_electrode_groups['03-01'].description\n",
    "# tet_electrode_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eeg_files = ns.get_eeg_by_day(eeg_path, prefix, 'eeg')\n",
    "lfp_data = []\n",
    "\n",
    "lfp = pynwb.ecephys.LFP(electrical_series=lfp_data)\n",
    "# read data from EEG/*eeg*.mat files and build TimeSeries object\n",
    "\n",
    "print('processing LFP data for day %2d' % day)\n",
    "for tet_num in tets.keys():\n",
    "    print(' -> tet_num: %d' % tet_num)\n",
    "    timestamps, data = ns.build_day_eeg(eeg_files[day][tet_num], eeg_samprate)\n",
    "    timestamps += NSpike_posixtime_offset\n",
    "    name = \"{prefix}eeg-{day}-{tet}\".format(prefix=prefix, day=day, tet=tet_num)\n",
    "    lfp.create_electrical_series(name=name, \n",
    "                                 data=data / 1000, # convert mV to V, as expected\n",
    "                                 electrodes=lfp_electrode_table_region[tet_num],\n",
    "                                 timestamps=timestamps)\n",
    "nwbf.add_acquisition(lfp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unit metadata first\n",
    "# External clustering software gives names for each cluster--we want to preserve these\n",
    "nwbf.add_unit_column('cluster_name',  '(str) cluster name from clustering software')\n",
    "\n",
    "# # For tetrode data, this will usually be all channels in the tetrode (2018Dec03--now in spec with 'electrodes' field?)\n",
    "# nwbf.add_unit_column('neighborhood',  '(electrodeTableRegion) list of electrodes on which spikes were clustered')\n",
    "\n",
    "# AKA 'Valid_times'--the times during which a spike from this cluster could have possibly been observed.\n",
    "# (handle periods between behavior epochs, acquisition system dropouts, etc.)\n",
    "nwbf.add_unit_column('obs_intervals', '(intervalSeries) Observation Intervals for the spike times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the spike times from the spikes files\n",
    "#each cluster gets a unique number starting at zero\n",
    "\n",
    "spike_files = ns.get_files_by_day(data_dir, prefix, 'spikes')\n",
    "print('\\nLoading spikes file :' + spike_files[day])\n",
    "mat = ns.loadmat_ff(spike_files[day], 'spikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_unit = []\n",
    "obs_intervals = {}\n",
    "cluster_by_tet = {}\n",
    "cluster_id = 0\n",
    "\n",
    "# Matlab structs are nested by: day, epoch, tetrode, cluster, but we will want to save all spikes from a give cluster\n",
    "# *across multiple epochs* in same spike list. So we rearrange the nested matlab structures for convenience. We \n",
    "# create a nested dict, keyed by 1) tetrode, 2) cluster number, then 3) epoch. NB the keys are 1-indexed, to be \n",
    "# consistent with the original data collection. (We only process one day at a time for now, so no need to nest days).\n",
    "\n",
    "spike_struct = mat[day]\n",
    "for epoch_num, espikes in spike_struct.items():\n",
    "    for tet_num, tspikes in espikes.items():\n",
    "        # respect tet subset selection done above\n",
    "        if tet_num not in tets.keys():\n",
    "            continue\n",
    "        if tet_num not in cluster_by_tet.keys():\n",
    "            cluster_by_tet[tet_num] = {}\n",
    "        for cluster_num, cspikes in tspikes.items():\n",
    "            if cluster_num not in cluster_by_tet[tet_num].keys():\n",
    "                cluster_by_tet[tet_num][cluster_num] = {}\n",
    "            cluster_by_tet[tet_num][cluster_num][epoch_num] = cspikes\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create the SpikeEventStructures and their containing EventWaveform objects\n",
    "colidx_timestamps = 0\n",
    "\n",
    "for tet_num in cluster_by_tet.keys():\n",
    "    obs_intervals[tet_num] = {}\n",
    "    for cluster_num in cluster_by_tet[tet_num].keys():\n",
    "        cluster_name = 'd%d t%d c%d' % (day, tet_num, cluster_num)\n",
    "        print('Adding cluster id: %3d, name: %s' % (cluster_id, cluster_name))\n",
    "\n",
    "        cluster_tmp = cluster_by_tet[tet_num][cluster_num]\n",
    "\n",
    "#         # construct a full data array and a parallel list of observation intervals\n",
    "#         obs_intervals[tet_num][cluster_num] = pynwb.misc.IntervalSeries(\n",
    "#                                                     name = cluster_name,\n",
    "#                                                     description = 'Observation intervals for spikes from cluster ' +\n",
    "#                                                     str(cluster_num) + ' on tetrode ' + str(tet_num))\n",
    "        obs_intervals[tet_num][cluster_num] = np.zeros([0,2])\n",
    "\n",
    "        spikes_ep = []\n",
    "        for epoch in cluster_tmp.keys():\n",
    "            if cluster_tmp[epoch]['data'].shape[0]:\n",
    "                spikes_ep.append(cluster_tmp[epoch]['data'][:,colidx_timestamps] + NSpike_posixtime_offset)\n",
    "            for obs_intervals_cl_ep in cluster_tmp[epoch]['timerange']:\n",
    "                # 'timerange' for each cell is given in NSpike timestamp units\n",
    "                obs_int = (obs_intervals_cl_ep.T.astype(float)/NSpike_timestamps_per_sec) + NSpike_posixtime_offset\n",
    "                obs_intervals[tet_num][cluster_num] = np.append(obs_intervals[tet_num][cluster_num], [obs_int], axis=0)\n",
    "                #                obs_intervals[tet_num][cluster_num].add_interval(*obs_int)\n",
    "\n",
    "        spiketimes = np.concatenate(spikes_ep)\n",
    "\n",
    "        # Add Observation Intervals to nwbfile willy-nilly (1 per cluster), \n",
    "        # so that we can successfully refer to them in the Unit metadata table\n",
    "#        spike_mod.add_data_interface(obs_intervals[tet_num][cluster_num])\n",
    "\n",
    "#         nwbf.add_unit(data = {'cluster_name': cluster_name, \n",
    "#                               'elec_group': tet_electrode_group[tet_num], \n",
    "#                               # can't just refer to electrode_table_region itself?: are never added to nwbfile to \n",
    "#                               # begin with. Instead, use 'data' field, which is a list of electrodeTable indices.\n",
    "# #                               'neighborhood': tet_electrode_table_region[tet_num], # tet_electrode_table_region[tet_num].data, \n",
    "#                               'obs_intervals': obs_intervals[tet_num][cluster_num]},\n",
    "#                       id = cluster_id)\n",
    "\n",
    "#         spike_UnitTimes.add_spike_times(cluster_id, spiketimes)\n",
    "\n",
    "        nwbf.add_unit(spike_times= spiketimes,\n",
    "                      electrodes=tet_electrode_table_region[tet_num].data, # wants list of electrode ids, not an electrodeTableRegion\n",
    "#                       electrode_group=[tet_electrode_group[tet_num]], # we can't figure out what is an acceptable input, here:\n",
    "#                                                                       # TypeError: incorrect type for 'container' (got 'list', expected 'Builder, Container or ReferenceBuilder')\n",
    "                      cluster_name=cluster_name, \n",
    "                      obs_intervals=obs_intervals[tet_num][cluster_num],\n",
    "                      id = cluster_id)\n",
    "\n",
    "        cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out NWBfile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an NWBFile\n",
    "with pynwb.NWBHDF5IO(nwb_filename, mode='w') as iow:\n",
    "    iow.write(nwbf)\n",
    "print('Wrote nwb file: ' + nwb_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nwbf.electrodes.id.data)\n",
    "print(nwbf.units.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our NWBfile, and check some roundtrip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf_read = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_id = 1\n",
    "print(nwbf_read.units.get_unit_spike_times(cl_id).shape)\n",
    "print(nwbf_read.units['cluster_name'][cl_id])\n",
    "print(nwbf_read.units['obs_intervals'][cl_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the electrical_series objects and work with it.\n",
    "example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[-1]\n",
    "nwbf_read.get_acquisition('LFP').electrical_series\n",
    "\n",
    "def fmt_sec_from_midnight (x, pos):\n",
    "    x_posix = mdates.num2epoch(x)\n",
    "    x_dt = mdates.num2date(x) \n",
    "    prev_midnight = x_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return (x_dt - prev_midnight).total_seconds()\n",
    "#    return \"%0.6f\" % mdates.num2epoch(x)\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,10))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "print(ax1.xaxis.major.formatter)\n",
    "xtick_locator = mticker.AutoLocator()\n",
    "xtick_formatter = mticker.FuncFormatter(qu.fmt_truncate_posix)\n",
    "# xtick_formatter = mdates.AutoDateFormatter(xtick_locator)\n",
    "# xtick_formatter = mticker.ScalarFormatter(xtick_locator)\n",
    "\n",
    "\n",
    "for i in [1]: #range(len(list(nwbf_read.get_acquisition('LFP').electrical_series.items()))):\n",
    "    example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[i]\n",
    "    nwbf_read.get_acquisition('LFP').electrical_series\n",
    "    #ts = (example_es.timestamps.value[-1001:-1]*1e9).astype('datetime64[ns]')\n",
    "    ts = example_es.timestamps.value[-10001:-1] \n",
    "    print(\"plot #%d\" % i)\n",
    "    ax1.plot(ts, example_es.data[0:10000] * 1000)\n",
    "\n",
    "ax1.set_title(example_es_name)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "# ax1.set_xlabel('Time (s; prefix = 113615\\u2026)')\n",
    "ax1.set_ylabel('Amplitude (mV)')\n",
    "ax1.xaxis.set_major_locator(xtick_locator)\n",
    "ax1.xaxis.set_major_formatter(xtick_formatter)\n",
    "print(ax1.xaxis.major.formatter)\n",
    "\n",
    "\n",
    "# xtick_locator = mdates.AutoDateLocator()\n",
    "\n",
    "\n",
    "#fig1.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

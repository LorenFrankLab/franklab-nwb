{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pynwb\n",
    "import nspike_helpers as ns \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug settings\n",
    "limit_num_of_tets = None # To speed up testing. Set to None to load all tets\n",
    "\n",
    "# Session-specific params\n",
    "data_dir = '/opt/data46/FrankData/kkay/Bon/'\n",
    "data_source = 'Animal Bond'\n",
    "anim = 'Bon' \n",
    "day = 4\n",
    "\n",
    "# 'Wall clock' (i.e. actual) date and time of the Nspike time = 0 for this experiment.\n",
    "# NOTE: this is not the actual zero_time, as we don't have easy access to that.\n",
    "dataset_zero_time = datetime(2006, 1, 1, 12, 0, 0, tzinfo=pytz.timezone('US/Pacific'))\n",
    "\n",
    "# General params/presets\n",
    "file_create_date = datetime.now()\n",
    "\n",
    "source = 'NSpike data acquisition system'\n",
    "eeg_samprate = 1500.0 # Hz\n",
    "\n",
    "eeg_subdir = \"EEG\"\n",
    "epochs_file = \"times.mat\"\n",
    "tetinfo_file = \"tetinfo.mat\"\n",
    "timestamps_per_sec = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse inputs and create NWBfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_str = '%02d' % day\n",
    "\n",
    "nwb_filename = anim + day_str + '_test.nwb'\n",
    "\n",
    "# check the input arguments\n",
    "if not os.path.exists(data_dir):\n",
    "        print('Error: data_dir %s does not exist' % data_dir)\n",
    "        exit(-1)\n",
    "\n",
    "# get filename prefix and file locations\n",
    "prefix = anim.lower()\n",
    "eeg_path = os.path.join(data_dir, eeg_subdir)\n",
    "\n",
    "# Calculate the POSIX timestamp when Nspike clock = 0 (seconds)\n",
    "Nspike_posixtime_offset = dataset_zero_time.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf = pynwb.NWBFile(data_source,\n",
    "               'Converted NSpike data from %s' % data_dir,\n",
    "               anim+day_str,\n",
    "               dataset_zero_time,\n",
    "               file_create_date=file_create_date,\n",
    "               lab='Frank Laboratory',\n",
    "               experimenter='Mattias Karlsson',\n",
    "               institution='UCSF',\n",
    "               experiment_description='Recordings from awake behaving rat',\n",
    "               session_id=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create position, direction and speed\n",
    "position_list = []\n",
    "pos = pynwb.behavior.Position(data_source, position_list, 'Position')\n",
    "\n",
    "direction_list = []\n",
    "dir = pynwb.behavior.CompassDirection(data_source, direction_list, 'Head Direction')\n",
    "\n",
    "speed_list = []\n",
    "speed = pynwb.behavior.BehavioralTimeSeries(data_source, speed_list, 'Speed')\n",
    "\n",
    "# NOTE that day_inds is 0 based\n",
    "time_list = {}\n",
    "nwb_epoch = {}\n",
    "pos_files = ns.get_files_by_day(data_dir, prefix, 'pos')\n",
    "task_files = ns.get_files_by_day(data_dir, prefix, 'task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat_ff(task_files[day], 'task')\n",
    "task_struct = mat[day]\n",
    "# find the pos file for this day and load it\n",
    "\n",
    "mat = ns.loadmat_ff(pos_files[day], 'pos')\n",
    "pos_struct = mat[day]\n",
    "\n",
    "# compile a list of time intervals in an array and create the position, head direction and velocity structures\n",
    "time_list = []\n",
    "\n",
    "# Assume field order: (time,x,y,dir,vel)\n",
    "(time_idx, x_idx, y_idx, dir_idx, vel_idx) = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num, pos_epoch in pos_struct.items():\n",
    "\n",
    "    # convert times to POSIX time\n",
    "    timestamps = pos_epoch['data'][:,time_idx] + Nspike_posixtime_offset\n",
    "\n",
    "    # TODO: create a shared TimeSeries for timestamps, across all behavioral timeseries\n",
    "    # ?? timestamps_obj = pynwb.TimeSeries(timestamps=timestamps...)\n",
    "\n",
    "    # collect times of epoch start and end\n",
    "    time_list.append([timestamps[0], timestamps[-1]])\n",
    "\n",
    "    m_per_pixel = pos_epoch['cmperpixel'][0,0]/100 # NWB wants meters per pixel\n",
    "\n",
    "    # we can also create new SpatialSeries for the position, direction and velocity information\n",
    "    #NOTE: Each new spatial series has to have a unique name.\n",
    "    pos.create_spatial_series(name='Position d%d e%d' % (day, epoch_num), \n",
    "                              source='overhead camera',\n",
    "                              timestamps = timestamps,\n",
    "                              data=pos_epoch['data'][:, (x_idx, y_idx)],\n",
    "                              reference_frame='corner of video frame',\n",
    "                              conversion=m_per_pixel,\n",
    "                              #unit='m'\n",
    "                              ) # *after* conversion\n",
    "\n",
    "    dir.create_spatial_series(name='Head Direction d%d e%d'% (day, epoch_num), \n",
    "                              source='overhead camera',\n",
    "                              timestamps=timestamps,\n",
    "                              data=pos_epoch['data'][:, dir_idx],\n",
    "                              reference_frame='0=facing top of video frame (?), positive clockwise (?)',\n",
    "                              #unit='radians'\n",
    "                              )\n",
    "\n",
    "    speed.create_timeseries(name='Speed d%d e%d' % (day, epoch_num),\n",
    "                             source='overhead camera',\n",
    "                             timestamps=timestamps,\n",
    "                             data=pos_epoch['data'][:, vel_idx],\n",
    "                             unit='m/s', # *after* conversion. data values are in pixels/s\n",
    "                             conversion=m_per_pixel,\n",
    "                             description='smoothed movement speed estimate')\n",
    "time_list = np.asarray(time_list)\n",
    "                                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Processing module for behavior\n",
    "behav_mod = nwbf.create_processing_module('Behavior', data_source, 'Behavioral variables')\n",
    "# add the position, direction and speed data\n",
    "behav_mod.add_data_interface(pos)\n",
    "behav_mod.add_data_interface(dir)\n",
    "behav_mod.add_data_interface(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs (Not currently implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list to store all of the fields in the task structure and \n",
    "# # a parallel list to store the created task interval structures\n",
    "# task_fields = []\n",
    "# task_intervals = []\n",
    "\n",
    "# # each day will be defined as a single Epoch in NWB so we go through and get the first and last time from the\n",
    "# # position data\n",
    "# day_start = time_list[0,0]\n",
    "# day_end = time_list[-1,1]\n",
    "\n",
    "# nwb_epoch = nwbf.create_epoch('day %s' % day, data_source, day_start, day_end, [], 'day %s' % day)\n",
    "# # add ignore intervals for the spaces between our epochs (it's not clear if this is necessary, but it won't hurt)\n",
    "# # also, there's probably a more \"python-ic\" way to do this, but I don't know what it is 8-)\n",
    "# if len(time_list) > 1:\n",
    "#         n = 1\n",
    "#         while n <= #NO!# len(time_list):\n",
    "#                 #nwb_epoch.add_ignore_interval(time_list[n-1][1], time_list[n][0])\n",
    "#                 n += 1\n",
    "\n",
    "# # now we go through the task structure and add a new interval series or an interval to an existing interval series\n",
    "# # for each element in the task structure\n",
    "# for epoch_num, task_epoch in enumerate(task_struct):\n",
    "#         if task_epoch.size > 0:\n",
    "#                 task_epoch = task_epoch[0,0] # retrieve dict from 1x1 ndarray\n",
    "#                 for field_name, value in task_epoch.items()\n",
    "#                         if field_name not in task_fields:\n",
    "#                                 # add the field_name to the list and create a new IntervalSeries for it\n",
    "#                                 task_fields.append(field_name)\n",
    "# #                               tmp_array = np.ndarray(2);\n",
    "# #                               tmp_array = [time_list[epoch_num][0], time_list[epoch_num][1]]\n",
    "#                                 tmp_interval = IntervalSeries(field_name, 'matlab task structure')\n",
    "#                                 # add the interval for this epoch\n",
    "#                                 tmp_interval.add_interval(*time_list[epoch_ind,:])\n",
    "#                                 task_intervals.append(tmp_interval)\n",
    "#                         else:\n",
    "#                                 # add the interval to appropriate element of the list\n",
    "#                                 task_intervals[task_fields.index(field_name)].add_interval(*time_list[epoch_ind])\n",
    "\n",
    "\n",
    "# # Now add the complete list of task intervals to the behav_mod module\n",
    "# for interval in task_intervals:\n",
    "#         behav_mod.add_data_interface(BehavioralEpochs('task information', interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetrode info\n",
    "Load in `tetinfo` struct and populate ElectrodeTable, electrode groups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the electrode table.\n",
    "# The logic here is as follows:\n",
    "#   Each Tetrode gets its own ElectrodeGroup and ElectrodeTableRegion\n",
    "#   Each individual recording channel gets its own row in nwbfile.electrodes\n",
    "\n",
    "# we first create the ElectrodeTable that all the electrodes will go into\n",
    "nchan_per_tetrode = 4 #these files all contain tetrodes, so we assume four channels\n",
    "tetinfo_filename = \"%s/%s%s\" % (data_dir, prefix, tetinfo_file)\n",
    "recording_device = nwbf.create_device('NSpike acquisition system', data_source)\n",
    "tet_electrode_group = {}\n",
    "tet_electrode_table_region = {}\n",
    "lfp_electrode_table_region = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat_ff(tetinfo_filename, 'tetinfo')\n",
    "#only look at first epoch because rest are duplicates\n",
    "tets = mat[day][1]\n",
    "\n",
    "# For debugging, limit number of tets to import\n",
    "subset_keys = sorted(tets.keys())[0:limit_num_of_tets]\n",
    "tets = {k:v for (k,v) in tets.items() if k in subset_keys}\n",
    "\n",
    "print(limit_num_of_tets)\n",
    "print(\"Using tetrode numbers:\")\n",
    "print(subset_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tets[1]['depth'][0,0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenny's data has a nested [day][epoch][tetrode] structure but duplicates the info across epochs, so we can just\n",
    "# use the first epoch for everything\n",
    "chan_num = 0 # this will hold an incrementing channel number for the entire day of data\n",
    "for tet_num, tet in tets.items():\n",
    "    #print('making electrode group for day %d, tet %d' % (day, tet_ind))\n",
    "    # go through the list of fields\n",
    "    hemisphere = '?'\n",
    "    # tet.area/.subarea are 1-d arrays of Unicode strings\n",
    "    area = str(tet['area'][0]) if 'area' in tet else '?' # h5py barfs on numpy.str_ type objects?\n",
    "    if 'sub_area' in tet: \n",
    "        sub_area = str(tet['sub_area'][0]) # h5py barfs on numpy.str_ type objects?\n",
    "        location = area + ' ' + sub_area\n",
    "    else:\n",
    "        sub_area = '?'\n",
    "        location = area \n",
    "\n",
    "    # tet.depth is a 1x1 cell array in tetinfo struct for some reason (multiple depths?)\n",
    "    # (which contains the expected 1x1 numeric array)\n",
    "    coord = [np.nan, np.nan, tet['depth'][0, 0][0, 0] / 12 / 80 * 25.4] if 'depth' in tet else [np.nan, np.nan, np.nan]\n",
    "    impedance = np.nan\n",
    "    filtering = 'unknown - likely 600Hz-6KHz'\n",
    "\n",
    "    channel_location = [location, location, location, location]\n",
    "    channel_coordinates = [coord, coord, coord, coord]\n",
    "    electrode_name = \"%02d-%02d\" % (day, tet_num)\n",
    "    description = \"tetrode {tet_num} located in {location} on day {day}\".format(tet_num=tet_num,\n",
    "                                                                               location=location,\n",
    "                                                                               day=day)\n",
    "\n",
    "    # we need to create an electrode group for this tetrode\n",
    "    tet_electrode_group[tet_num] = nwbf.create_electrode_group(electrode_name,\n",
    "                                                        data_source,\n",
    "                                                        description,\n",
    "                                                        location,\n",
    "                                                        recording_device)\n",
    "\n",
    "    for i in range(nchan_per_tetrode):\n",
    "            # now add an electrode\n",
    "            nwbf.add_electrode(x = coord[0],\n",
    "                               y = coord[1],\n",
    "                               z = coord[2],\n",
    "                               imp = impedance,\n",
    "                               location = location,\n",
    "                               filtering = filtering,\n",
    "                               group = tet_electrode_group[tet_num],\n",
    "                               group_name = tet_electrode_group[tet_num].name,\n",
    "                               id = chan_num)\n",
    "            chan_num = chan_num + 1\n",
    "\n",
    "    # now that we've created four entries, one for each channel of the tetrode, we create a new\n",
    "    # electrode table region for this tetrode and number it appropriately\n",
    "    table_region_description = 'ntrode %d region' % tet_num\n",
    "    table_region_name = '%d' % tet_num\n",
    "    tet_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        list(range(chan_num-nchan_per_tetrode,chan_num)),\n",
    "        table_region_description,\n",
    "        table_region_name)\n",
    "\n",
    "    # Also create electrode_table_regions for each tetrode's LFP recordings\n",
    "    # (Assume that LFP is taken from the first channel)\n",
    "    lfp_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        [chan_num-nchan_per_tetrode],\n",
    "        table_region_description,\n",
    "        table_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tet_electrode_table_region[1].region\n",
    "# nwbf.ec_electrode_groups['03-01'].description\n",
    "# tet_electrode_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eeg_files = ns.get_eeg_by_day(eeg_path, prefix, 'eeg')\n",
    "lfp_data = []\n",
    "\n",
    "lfp = pynwb.ecephys.LFP(data_source, lfp_data)\n",
    "# read data from EEG/*eeg*.mat files and build TimeSeries object\n",
    "\n",
    "print('processing LFP data for day %2d' % day)\n",
    "for tet_num in tets.keys():\n",
    "    print(' -> tet_num: %d' % tet_num)\n",
    "    timestamps, data = ns.build_day_eeg(eeg_files[day][tet_num], eeg_samprate)\n",
    "    # convert the timestamps to POSIX time:\n",
    "    timestamps += Nspike_posixtime_offset\n",
    "    name = \"{prefix}eeg-{day}-{tet}\".format(prefix=prefix, day=day, tet=tet_num)\n",
    "    lfp.create_electrical_series(name, \n",
    "                                 source,\n",
    "                                 data / 1000, # convert mV to V, as expected\n",
    "                                 lfp_electrode_table_region[tet_num],\n",
    "                                 timestamps=timestamps)\n",
    "#add the lfp data to the file\n",
    "nwbf.add_acquisition(lfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unit metadata first\n",
    "# External clustering software gives names for each cluster--we want to preserve these\n",
    "nwbf.add_unit_column('cluster_name',  '(str) cluster name from clustering software')\n",
    "nwbf.add_unit_column('elec_group',    '(electrodeGroup) nTrode on which spikes were recorded')\n",
    "# For tetrode data, this will usually be all channels in the tetrode\n",
    "nwbf.add_unit_column('neighborhood',  '(electrodeTableRegion) list of electrodes on which spikes were clustered')\n",
    "# AKA 'Valid_times'--the times during which a spike from this cluster could have possibly been observed.\n",
    "# (handle periods between behavior epochs, acquisition system dropouts, etc.)\n",
    "nwbf.add_unit_column('obs_intervals', '(intervalSeries) Observation Intervals for the spike times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the spike times from the spikes files\n",
    "#each cluster gets a unique number starting at zero\n",
    "\n",
    "spike_files = ns.get_files_by_day(data_dir, prefix, 'spikes')\n",
    "print('\\nLoading spikes file :' + spike_files[day])\n",
    "mat = ns.loadmat_ff(spike_files[day], 'spikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_mod = nwbf.create_processing_module('Spike Data', data_source, 'Clustered Spikes')\n",
    "spike_UnitTimes = pynwb.misc.UnitTimes(data_source)\n",
    "\n",
    "spike_unit = []\n",
    "obs_intervals = {}\n",
    "cluster_by_tet = {}\n",
    "cluster_id = 0\n",
    "\n",
    "# Matlab structs are nested by: day, epoch, tetrode, cluster, but we will want to save all spikes from a give cluster\n",
    "# *across multiple epochs* in same spike list. So we rearrange the nested matlab structures for convenience. We \n",
    "# create a nested dict, keyed by 1) tetrode, 2) cluster number, then 3) epoch. NB the keys are 1-indexed, to be \n",
    "# consistent with the original data collection. (We only process one day at a time for now, so no need to nest days).\n",
    "\n",
    "spike_struct = mat[day]\n",
    "for epoch_num, espikes in spike_struct.items():\n",
    "    for tet_num, tspikes in espikes.items():\n",
    "        # respect tet subset selection done above\n",
    "        if tet_num not in tets.keys():\n",
    "            continue\n",
    "        if tet_num not in cluster_by_tet.keys():\n",
    "            cluster_by_tet[tet_num] = {}\n",
    "        for cluster_num, cspikes in tspikes.items():\n",
    "            if cluster_num not in cluster_by_tet[tet_num].keys():\n",
    "                cluster_by_tet[tet_num][cluster_num] = {}\n",
    "            cluster_by_tet[tet_num][cluster_num][epoch_num] = cspikes\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create the SpikeEventStructures and their containing EventWaveform objects\n",
    "for tet_num in cluster_by_tet.keys():\n",
    "    obs_intervals[tet_num] = {}\n",
    "    for cluster_num in cluster_by_tet[tet_num].keys():\n",
    "        print('Adding cluster id %d' % cluster_id)\n",
    "\n",
    "        cluster_name = 'd%d t%d c%d' % (day, tet_num, cluster_num)\n",
    "        print('cluster name: ' + cluster_name)\n",
    "\n",
    "        cluster_tmp = cluster_by_tet[tet_num][cluster_num]\n",
    "\n",
    "        # construct a full data array and a parallel list of observation intervals\n",
    "        obs_intervals[tet_num][cluster_num] = pynwb.misc.IntervalSeries(name = cluster_name, \n",
    "                                                    source = source,\n",
    "                                                    description = 'Observation intervals for spikes from cluster ' +\n",
    "                                                    str(cluster_num) + ' on tetrode ' + str(tet_num))\n",
    "\n",
    "        spikes_ep = []\n",
    "        for epoch in cluster_tmp.keys():\n",
    "            if cluster_tmp[epoch]['data'].shape[0]:\n",
    "                spikes_ep.append(cluster_tmp[epoch]['data'][:,0])\n",
    "            for obs_intervals_cl_ep in cluster_tmp[epoch]['timerange']:\n",
    "                # 'timerange' for each cell is given in NSpike timestamp units\n",
    "                obs_intervals[tet_num][cluster_num].add_interval(*obs_intervals_cl_ep.T.astype(float)/timestamps_per_sec)\n",
    "\n",
    "        spiketimes = np.concatenate(spikes_ep)\n",
    "\n",
    "        # Add Observation Intervals to nwbfile willy-nilly (1 per cluster), \n",
    "        # so that we can successfully refer to them in the Unit metadata table\n",
    "        spike_mod.add_data_interface(obs_intervals[tet_num][cluster_num])\n",
    "\n",
    "        nwbf.add_unit(data = {'cluster_name': cluster_name, \n",
    "                              'elec_group': tet_electrode_group[tet_num], \n",
    "                              # can't just refer to electrode_table_region itself: are never added to nwbfile to \n",
    "                              # begin with. Instead, use 'data' field, which is a list of electrodeTable indices.\n",
    "                              'neighborhood': lfp_electrode_table_region[tet_num], # tet_electrode_table_region[tet_num].data, \n",
    "                              'obs_intervals': obs_intervals[tet_num][cluster_num]},\n",
    "                      id = cluster_id)\n",
    "\n",
    "        spike_UnitTimes.add_spike_times(cluster_id, spiketimes)\n",
    "\n",
    "        cluster_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UnitTimes to the spike_mod ProcessingModule\n",
    "spike_mod.add_data_interface(spike_UnitTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out NWBfile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an NWBFile\n",
    "with pynwb.NWBHDF5IO(nwb_filename, mode='w') as iow:\n",
    "    iow.write(nwbf)\n",
    "print('Wrote nwb file: ' + nwb_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our NWBfile, and check some roundtrip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf_read = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_id = 1\n",
    "print(nwbf_read.modules['Spike Data']['UnitTimes'].get_unit_spike_times(cl_id).shape)\n",
    "clname_idx = nwbf_read.units.colnames.index('cluster_name')\n",
    "obsint_idx = nwbf_read.units.colnames.index('obs_intervals')\n",
    "print(nwbf_read.units.columns[clname_idx][cl_id])\n",
    "print(nwbf_read.units.columns[obsint_idx][cl_id].timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the electrical_series objects and work with it.\n",
    "example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[-1]\n",
    "nwbf_read.get_acquisition('LFP').electrical_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(len(list(nwbf_read.get_acquisition('LFP').electrical_series.items()))):\n",
    "    example_es_name, example_es = list(nwbf_read.get_acquisition('LFP').electrical_series.items())[i]\n",
    "    nwbf_read.get_acquisition('LFP').electrical_series\n",
    "    plt.plot(example_es.timestamps[0:10000], example_es.data[0:10000] * 1000)\n",
    "    plt.title(example_es_name)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude (mV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pynwb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import nspike_helpers as ns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session-specific import parameters\n",
    "File paths, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug settings\n",
    "limit_num_of_tets = 3 # To speed up testing. Set to None to load all tets\n",
    "\n",
    "# Session-specific params\n",
    "data_dir = '/opt/data46/FrankData/kkay/Bon/'\n",
    "nwb_filename = 'bon03_eegtest.nwb'\n",
    "data_source = 'Animal Bond'\n",
    "anim = 'Bon' \n",
    "day = 3\n",
    "day_str = '03'\n",
    "\n",
    "# 'Wall clock' (i.e. actual) date and time of the Nspike time = 0 for this experiment.\n",
    "# NOTE: this is not the actual zero_time, as we don't have easy access to that.\n",
    "dataset_zero_time = datetime(2006, 1, 1, 12, 0, 0, tzinfo=pytz.timezone('US/Pacific'))\n",
    "\n",
    "# General params/presets\n",
    "file_create_date = datetime.now()\n",
    "\n",
    "source = 'NSpike data acquisition system'\n",
    "eeg_samprate = 1500.0 # Hz\n",
    "\n",
    "eeg_subdir = \"EEG\"\n",
    "epochs_file = \"times.mat\"\n",
    "tetinfo_file = \"tetinfo.mat\"\n",
    "timestamps_per_sec = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse inputs and create NWBfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the input arguments\n",
    "if not os.path.exists(data_dir):\n",
    "        print('Error: data_dir %s does not exist' % data_dir)\n",
    "        exit(-1)\n",
    "\n",
    "# get filename prefix and file locations\n",
    "prefix = anim.lower()\n",
    "eeg_path = os.path.join(data_dir, eeg_subdir)\n",
    "\n",
    "# Calculate the POSIX timestamp when Nspike clock = 0 (seconds)\n",
    "Nspike_posixtime_offset = dataset_zero_time.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf = pynwb.NWBFile(data_source,\n",
    "               'Converted NSpike data from %s' % data_dir,\n",
    "               anim+day_str,\n",
    "               dataset_zero_time,\n",
    "               file_create_date=file_create_date,\n",
    "               lab='Frank Laboratory',\n",
    "               experimenter='Mattias Karlsson',\n",
    "               institution='UCSF',\n",
    "               experiment_description='Recordings from awake behaving rat',\n",
    "               session_id=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create position, direction and speed\n",
    "position_list = list()\n",
    "pos = pynwb.behavior.Position(data_source, position_list)\n",
    "\n",
    "direction_list = list()\n",
    "dir = pynwb.behavior.CompassDirection(data_source, direction_list)\n",
    "\n",
    "speed_list = list()\n",
    "speed = pynwb.behavior.BehavioralTimeSeries(data_source, speed_list)\n",
    "\n",
    "# NOTE that day_inds is 0 based\n",
    "time_list = dict()\n",
    "nwb_epoch = dict()\n",
    "pos_files = ns.get_files_by_day(data_dir, prefix, 'pos')\n",
    "task_files = ns.get_files_by_day(data_dir, prefix, 'task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat2(task_files[day])\n",
    "task_struct = mat['task'][0,day - 1]\n",
    "# find the pos file for this day and load it\n",
    "\n",
    "mat = ns.loadmat2(pos_files[day])\n",
    "pos_struct = mat['pos'][0,day-1][0,:] # pos_struct is a row vector, 1 cell per epoch\n",
    "\n",
    "# compile a list of time intervals in an array and create the position, head direction and velocity structures\n",
    "time_list = [] #np.zeros((0,2))\n",
    "time_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_ind, pos_epoch in enumerate(pos_struct):\n",
    "        epoch_num = epoch_ind + 1\n",
    "        if pos_epoch.size > 0:\n",
    "                pos_epoch = pos_epoch[0,0] # retrieve dict from 1x1 ndarray\n",
    "            \n",
    "                # Assume field order: (time,x,y,dir,vel)\n",
    "                (time_idx, x_idx, y_idx, dir_idx, vel_idx) = range(5)\n",
    "\n",
    "                # convert times to POSIX time\n",
    "                timestamps = pos_epoch['data'][:,time_idx] + Nspike_posixtime_offset\n",
    "                \n",
    "                # TODO: create a shared TimeSeries for timestamps, across all behavioral timeseries\n",
    "                # ?? timestamps_obj = pynwb.TimeSeries(timestamps=timestamps...)\n",
    "                \n",
    "                # collect times of epoch start and end\n",
    "                time_list.append([timestamps[0], timestamps[-1]])\n",
    "                \n",
    "                m_per_pixel = float(pos_epoch['cmperpixel'])/100 # NWB wants meters per pixel\n",
    "                \n",
    "                # we can also create new SpatialSeries for the position, direction and velocity information\n",
    "                #NOTE: Each new spatial series has to have a unique name.\n",
    "                pos.create_spatial_series(name='Position d%d e%d' % (day, epoch_num), \n",
    "                                          source='overhead camera',\n",
    "                                          timestamps = timestamps,\n",
    "                                          data=pos_epoch['data'][:, (x_idx, y_idx)],\n",
    "                                          reference_frame='corner of video frame',\n",
    "                                          conversion=m_per_pixel,\n",
    "                                          #unit='m'\n",
    "                                          ) # *after* conversion\n",
    "\n",
    "                dir.create_spatial_series(name='Head Direction d%d e%d'% (day, epoch_num), \n",
    "                                          source='overhead camera',\n",
    "                                          timestamps=timestamps,\n",
    "                                          data=pos_epoch['data'][:, dir_idx],\n",
    "                                          reference_frame='0=facing top of video frame (?), positive clockwise (?)',\n",
    "                                          #unit='radians'\n",
    "                                          )\n",
    "                \n",
    "                speed.create_timeseries(name='Speed d%d e%d' % (day, epoch_num),\n",
    "                                         source='overhead camera',\n",
    "                                         timestamps=timestamps,\n",
    "                                         data=pos_epoch['data'][:, vel_idx],\n",
    "                                         unit='m/s', # *after* conversion. data values are in pixels/s\n",
    "                                         conversion=m_per_pixel,\n",
    "                                         description='smoothed movement speed estimate')\n",
    "time_list = np.asarray(time_list)\n",
    "                                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Processing module for behavior\n",
    "behav_mod = nwbf.create_processing_module('Behavior', data_source, 'Behavioral variables')\n",
    "# add the position, direction and speed data\n",
    "behav_mod.add_data_interface(pos)\n",
    "behav_mod.add_data_interface(dir)\n",
    "behav_mod.add_data_interface(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs (Not currently implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list to store all of the fields in the task structure and \n",
    "# # a parallel list to store the created task interval structures\n",
    "# task_fields = list()\n",
    "# task_intervals = list()\n",
    "\n",
    "# # each day will be defined as a single Epoch in NWB so we go through and get the first and last time from the\n",
    "# # position data\n",
    "# day_start = time_list[0,0]\n",
    "# day_end = time_list[-1,1]\n",
    "\n",
    "# nwb_epoch = nwbf.create_epoch('day %s' % day, data_source, day_start, day_end, [], 'day %s' % day)\n",
    "# # add ignore intervals for the spaces between our epochs (it's not clear if this is necessary, but it won't hurt)\n",
    "# # also, there's probably a more \"python-ic\" way to do this, but I don't know what it is 8-)\n",
    "# if len(time_list) > 1:\n",
    "#         n = 1\n",
    "#         while n <= #NO!# len(time_list):\n",
    "#                 #nwb_epoch.add_ignore_interval(time_list[n-1][1], time_list[n][0])\n",
    "#                 n += 1\n",
    "\n",
    "# # now we go through the task structure and add a new interval series or an interval to an existing interval series\n",
    "# # for each element in the task structure\n",
    "# for epoch_num, task_epoch in enumerate(task_struct):\n",
    "#         if task_epoch.size > 0:\n",
    "#                 task_epoch = task_epoch[0,0] # retrieve dict from 1x1 ndarray\n",
    "#                 for field_name, value in task_epoch.items()\n",
    "#                         if field_name not in task_fields:\n",
    "#                                 # add the field_name to the list and create a new IntervalSeries for it\n",
    "#                                 task_fields.append(field_name)\n",
    "# #                               tmp_array = np.ndarray(2);\n",
    "# #                               tmp_array = [time_list[epoch_num][0], time_list[epoch_num][1]]\n",
    "#                                 tmp_interval = IntervalSeries(field_name, 'matlab task structure')\n",
    "#                                 # add the interval for this epoch\n",
    "#                                 tmp_interval.add_interval(*time_list[epoch_ind,:])\n",
    "#                                 task_intervals.append(tmp_interval)\n",
    "#                         else:\n",
    "#                                 # add the interval to appropriate element of the list\n",
    "#                                 task_intervals[task_fields.index(field_name)].add_interval(*time_list[epoch_ind])\n",
    "\n",
    "\n",
    "# # Now add the complete list of task intervals to the behav_mod module\n",
    "# for interval in task_intervals:\n",
    "#         behav_mod.add_data_interface(BehavioralEpochs('task information', interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tetrode info\n",
    "Load in `tetinfo` struct and populate ElectrodeTable, electrode groups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the electrode table.\n",
    "# The logic here is as follows:\n",
    "#   Each Tetrode gets its own ElectrodeGroup and ElectrodeTableRegion\n",
    "#   Each individual recording channel gets its own row in nwbfile.electrodes\n",
    "\n",
    "# we first create the ElectrodeTable that all the electrodes will go into\n",
    "nchan_per_tetrode = 4 #these files all contain tetrodes, so we assume four channels\n",
    "tetinfo_filename = \"%s/%s%s\" % (data_dir, prefix, tetinfo_file)\n",
    "recording_device = nwbf.create_device('NSpike acquisition system', data_source)\n",
    "tet_electrode_group = dict()\n",
    "tet_electrode_table_region = dict()\n",
    "lfp_electrode_table_region = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ns.loadmat2(tetinfo_filename)\n",
    "# tetinfo = mat['tetinfo'][0, day-1][0, 0].squeeze(axis=0)\n",
    "\n",
    "# for (i,t) in enumerate(tetinfo):\n",
    "#     print(i)\n",
    "#     print(type(t))\n",
    "#     print(t.shape)\n",
    "#     print(type(t[0, 0]['area'][0]))\n",
    "#     print(t[0, 0]['area'].dtype)\n",
    "# #     print(t[0, 0]['area'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict using 1-indexed tetrode numbers (pretty names)\n",
    "tet_structs = mat['tetinfo'][0,day-1][0,0].squeeze(axis=0) #only look at first epoch because rest are duplicates\n",
    "tets = {i+1:tet_struct[0,0] for (i,tet_struct) in enumerate(tet_structs) if tet_struct.size > 0}\n",
    "\n",
    "# For debugging, limit number of tets to import\n",
    "subset_keys = sorted(tets.keys())[0:limit_num_of_tets]\n",
    "tets = {k:v for (k,v) in tets.items() if k in subset_keys}\n",
    "\n",
    "print(limit_num_of_tets)\n",
    "print(\"Using tetrode numbers:\")\n",
    "print(subset_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenny's data has a nested [day][epoch][tetrode] structure but duplicates the info across epochs, so we can just\n",
    "# use the first epoch for everything\n",
    "chan_num = 0 # this will hold an incrementing channel number for the entire day of data\n",
    "for tet_num, tet in tets.items():\n",
    "        #print('making electrode group for day %d, tet %d' % (day, tet_ind))\n",
    "        # go through the list of fields\n",
    "        hemisphere = '?'\n",
    "        # tet.area/.subarea are 1-d arrays of Unicode strings\n",
    "        area = str(tet['area'][0]) if 'area' in tet else '?' # h5py barfs on numpy.str_ type objects?\n",
    "        if 'sub_area' in tet: \n",
    "            sub_area = str(tet['sub_area'][0]) # h5py barfs on numpy.str_ type objects?\n",
    "            location = area + ' ' + sub_area\n",
    "        else:\n",
    "            sub_area = '?'\n",
    "            location = area \n",
    "\n",
    "        # tet.depth is a 1x1 cell array in tetinfo struct for some reason (multiple depths?)\n",
    "        # (which contains the expected 1x1 numeric array)\n",
    "        coord = [np.nan, np.nan, tet['depth'][0, 0][0, 0] / 12 / 80 * 25.4] if 'depth' in tet else [np.nan, np.nan, np.nan]\n",
    "        impedance = np.nan\n",
    "        filtering = 'unknown - likely 600Hz-6KHz'\n",
    "\n",
    "        channel_location = [location, location, location, location]\n",
    "        channel_coordinates = [coord, coord, coord, coord]\n",
    "        electrode_name = \"%02d-%02d\" % (day, tet_num)\n",
    "        description = \"tetrode {tet_num} located in {location} on day {day}\".format(tet_num=tet_num,\n",
    "                                                                                   location=location,\n",
    "                                                                                   day=day)\n",
    "\n",
    "        # we need to create an electrode group for this tetrode\n",
    "        tet_electrode_group[tet_num] = nwbf.create_electrode_group(electrode_name,\n",
    "                                                            data_source,\n",
    "                                                            description,\n",
    "                                                            location,\n",
    "                                                            recording_device)\n",
    "\n",
    "        for i in range(nchan_per_tetrode):\n",
    "                # now add an electrode\n",
    "                nwbf.add_electrode(x = coord[0],\n",
    "                                   y = coord[1],\n",
    "                                   z = coord[2],\n",
    "                                   imp = impedance,\n",
    "                                   location = location,\n",
    "                                   filtering = filtering,\n",
    "                                   group = tet_electrode_group[tet_num],\n",
    "                                   group_name = tet_electrode_group[tet_num].name,\n",
    "                                   id = chan_num)\n",
    "                chan_num = chan_num + 1\n",
    "\n",
    "        # now that we've created four entries, one for each channel of the tetrode, we create a new\n",
    "        # electrode table region for this tetrode and number it appropriately\n",
    "        table_region_description = 'ntrode %d region' % tet_num\n",
    "        table_region_name = '%d' % tet_num\n",
    "        tet_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "            list(range(chan_num-nchan_per_tetrode,chan_num)),\n",
    "            table_region_description,\n",
    "            table_region_name)\n",
    "\n",
    "        # Also create electrode_table_regions for each tetrode's LFP recordings\n",
    "        # (Assume that LFP is taken from the first channel)\n",
    "        lfp_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "            [chan_num-nchan_per_tetrode],\n",
    "            table_region_description,\n",
    "            table_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tet_electrode_table_region[1].region\n",
    "# nwbf.ec_electrode_groups['03-01'].description\n",
    "tet_electrode_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the (unique) electrodeTable for this file\n",
    "# et = nwbf.get_acquisition('LFP').electrical_series['boneeg-3-1']\n",
    "# # query for rows in table with known group name (day-tetnum, in this case)\n",
    "# # eti = et.which(group_name='03-01')\n",
    "# # do a list comprehension to get the tuples\n",
    "# # et_result = [nwbf.ec_electrodes.data[idx] for idx in eti] # list comprehension\n",
    "# # get electrodeTable column index for field 'group'\n",
    "# # colidx_group = et.columns.index('group')\n",
    "\n",
    "# # print(et.data[eti[0]][colidx_group]) # electrodeGroup for tet 3-10\n",
    "# # print(et_result[0][colidx_group])   # same as above\n",
    "\n",
    "# print(et) # NB no 'fields', why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eeg_files = ns.get_eeg_by_day(eeg_path, prefix, 'eeg')\n",
    "#LFP data\n",
    "lfp_data = list()\n",
    "lfp = pynwb.ecephys.LFP(data_source, lfp_data)\n",
    "# read data from EEG/*eeg*.mat files and build TimeSeries object\n",
    "\n",
    "print('processing LFP data for day %2d' % day)\n",
    "dayfiles = eeg_files[day]\n",
    "for tet_num in tets.keys():\n",
    "    print(' -> tet_num: %d' % tet_num)\n",
    "    timestamps, data = ns.build_day_eeg(dayfiles[tet_num], eeg_samprate)\n",
    "    # convert the timestamps to POSIX time:\n",
    "    timestamps += Nspike_posixtime_offset\n",
    "    name = \"{prefix}eeg-{day}-{tet}\".format(prefix=prefix, day=day, tet=tet_num)\n",
    "\n",
    "    #lfp_data.append(ElectricalSeries(name, source, data, electrode_table_region[day][tet_num], starttime=0,\n",
    "    #                                                                rate=eeg_samprate, timestamps=timestamps))\n",
    "    #     lfp.create_electrical_series(name, source, data, electrode_table_region[tet_num], starting_time=0.0,\n",
    "    #                                                          rate=eeg_samprate, timestamps=timestamps)\n",
    "    print(str(timestamps.dtype) + ' ' + str(data.dtype))\n",
    "    lfp.create_electrical_series(name, source, data, lfp_electrode_table_region[tet_num], timestamps=timestamps)\n",
    "print('processed LFP data from day %d' % day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the lfp data to the file\n",
    "nwbf.add_acquisition(lfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unit metadata first\n",
    "# External clustering software gives names for each cluster--we want to preserve these\n",
    "nwbf.add_unit_column('cluster_name',  '(str) cluster name from clustering software')\n",
    "nwbf.add_unit_column('elec_group',    '(electrodeGroup) nTrode on which spikes were recorded')\n",
    "# For tetrode data, this will usually be all channels in the tetrode\n",
    "nwbf.add_unit_column('neighborhood',  '(electrodeTableRegion) list of electrodes on which spikes were clustered')\n",
    "# AKA 'Valid_times'--the times during which a spike from this cluster could have possibly been observed.\n",
    "# (handle periods between behavior epochs, acquisition system dropouts, etc.)\n",
    "nwbf.add_unit_column('obs_intervals', '(intervalSeries) Observation Intervals for the spike times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the spike times from the spikes files\n",
    "#each cluster gets a unique number starting at zero\n",
    "\n",
    "spike_files = ns.get_files_by_day(data_dir, prefix, 'spikes')\n",
    "print('\\nLoading spikes file :' + spike_files[day])\n",
    "mat = ns.loadmat2(spike_files[day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_mod = nwbf.create_processing_module('Spike Data', data_source, 'Clustered Spikes')\n",
    "spike_UnitTimes = pynwb.misc.UnitTimes(data_source)\n",
    "\n",
    "spike_unit = []\n",
    "obs_intervals = {}\n",
    "cluster_by_tet = {}\n",
    "cluster_id = 0\n",
    "\n",
    "# Matlab structs are nested by: day, epoch, tetrode, cluster, but we will want to save all spikes from a give cluster\n",
    "# *across multiple epochs* in same spike list. So we rearrange the nested matlab structures for convenience. We \n",
    "# create a nested dict, keyed by 1) tetrode, 2) cluster number, then 3) epoch. NB the keys are 1-indexed, to be \n",
    "# consistent with the original data collection. (We only process one day at a time for now, so no need to nest days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_struct = mat['spikes'][0,day - 1].T # spike_struct is a row vector, 1 Matlab 'cell' per epoch\n",
    "for epoch_ind, espikes in enumerate(spike_struct):\n",
    "        espikes = espikes[0].T # espikes is a row vector, 1 Matlab 'cell' per tetrode \n",
    "        # use epoch_ind + 1 to keep the numbers consistent with the original data collection\n",
    "        epoch_num = epoch_ind + 1\n",
    "        for tet_ind, tspikes in enumerate(espikes):\n",
    "                tspikes = tspikes[0].T # tspikes is a row vector, 1 Matlab 'cell' per cluster \n",
    "                # use tet_ind + 1 to keep the numbers consistent with the original data collection\n",
    "                tet_num = tet_ind + 1\n",
    "                # respect tet subset selection done above\n",
    "                if tet_num not in tets.keys():\n",
    "                        continue\n",
    "                if tet_num not in cluster_by_tet.keys():\n",
    "                        cluster_by_tet[tet_num] = dict()\n",
    "                for cluster_ind, cspikes in enumerate(tspikes):\n",
    "                        cspikes = cspikes[0]\n",
    "                        cluster_num = cluster_ind+1 # keep numbers consistent with original data collection\n",
    "                        # check to see if there is something in the structure\n",
    "                        if len(cspikes):\n",
    "                                if cluster_num not in cluster_by_tet[tet_num].keys():\n",
    "                                        cluster_by_tet[tet_num][cluster_num] = dict()\n",
    "                                cluster_by_tet[tet_num][cluster_num][epoch_num] = cspikes[0,0]\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create the SpikeEventStructures and their containing EventWaveform objects\n",
    "for tet_num in cluster_by_tet.keys():\n",
    "        obs_intervals[tet_num] = {}\n",
    "        for cluster_num in cluster_by_tet[tet_num].keys():\n",
    "                print('Adding cluster id %d' % cluster_id)\n",
    "\n",
    "                cluster_name = 'd%d t%d c%d' % (day, tet_num, cluster_num)\n",
    "                print('cluster name: ' + cluster_name)\n",
    "                \n",
    "                cluster_tmp = cluster_by_tet[tet_num][cluster_num]\n",
    "                \n",
    "                # construct a full data array and a parallel list of observation intervals\n",
    "                obs_intervals[tet_num][cluster_num] = pynwb.misc.IntervalSeries(name = cluster_name, \n",
    "                                                            source = source,\n",
    "                                                            description = 'Observation intervals for spikes from cluster ' +\n",
    "                                                            str(cluster_num) + ' on tetrode ' + str(tet_num))\n",
    "                \n",
    "                spikes_ep = []\n",
    "                for epoch in cluster_tmp.keys():\n",
    "                        if cluster_tmp[epoch]['data'].shape[0]:\n",
    "                                spikes_ep.append(cluster_tmp[epoch]['data'][:,0])\n",
    "                        for obs_intervals_cl_ep in cluster_tmp[epoch]['timerange']:\n",
    "                                obs_intervals[tet_num][cluster_num].add_interval(*obs_intervals_cl_ep.T.astype(float))\n",
    "    \n",
    "                spiketimes = np.concatenate(spikes_ep)\n",
    "                \n",
    "                # Add Observation Intervals to nwbfile willy-nilly (1 per cluster), \n",
    "                # so that we can successfully refer to them in the Unit metadata table\n",
    "                spike_mod.add_data_interface(obs_intervals[tet_num][cluster_num])\n",
    "                \n",
    "                nwbf.add_unit(data = {'cluster_name': cluster_name, \n",
    "                                      'elec_group': tet_electrode_group[tet_num], \n",
    "                                      # can't just refer to electrode_table_region itself: are never added to nwbfile to \n",
    "                                      # begin with. Instead, use 'data' field, which is a list of electrodeTable indices.\n",
    "                                      'neighborhood': lfp_electrode_table_region[tet_num], # tet_electrode_table_region[tet_num].data, \n",
    "                                      'obs_intervals': obs_intervals[tet_num][cluster_num]},\n",
    "                              id = cluster_id)\n",
    "                \n",
    "                spike_UnitTimes.add_spike_times(cluster_id, spiketimes)\n",
    "                                \n",
    "                cluster_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UnitTimes to the spike_mod ProcessingModule\n",
    "spike_mod.add_data_interface(spike_UnitTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out NWBfile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an NWBFile\n",
    "with pynwb.NWBHDF5IO(nwb_filename, mode='w') as iow:\n",
    "    iow.write(nwbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our NWBfile, and check some roundtrip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf_read = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf_read.get_acquisition('LFP').electrical_series['boneeg-3-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_id = 1\n",
    "print(nwbf_read.modules['Spike Data']['UnitTimes'].get_unit_spike_times(cl_id).shape)\n",
    "clname_idx = nwbf_read.units.colnames.index('cluster_name')\n",
    "obsint_idx = nwbf_read.units.colnames.index('obs_intervals')\n",
    "print(nwbf_read.units.columns[clname_idx][cl_id])\n",
    "print(nwbf_read.units.columns[obsint_idx][cl_id].timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = nwbf_read.get_acquisition('LFP').electrical_series['boneeg-3-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(es.timestamps[0:10000], es.data[0:10000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

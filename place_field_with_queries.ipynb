{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sneak Peak: Frank Lab place field analysis using NWB 2.0 + Query Framework (prototype)\n",
    "This is an sneak peak of an approach for querying and analyzing NWB 2.0 files in the Frank Lab. We would like to build a Query Framework for efficiently working with data stored in NWB files, including eventually querying across days, animals, and even experiments. As a first step toward this goal,\n",
    "we show some canonical Frank Lab queries and analyses using a single day of an animal from one experiment. \n",
    "\n",
    "In addition to vanilla NWB, we use Frank Lab NWB extensions for representing behavioral tasks and apparatuses. See 'create_franklab_nwbfile.ipynb' for a walk-through of how we create these NWB files. Running that notebook first will create an NWB file that works with this notebook.\n",
    "\n",
    "We have also developed helper classes (nwb_query.py) for effectively querying data, especially time queries (i.e. selecting subsets of data in particular intervals of time). Importantly, these classes keep track of the valid intervals (i.e. observation intervals) over which the data are valid/observed. In fact, we require all time-based data (e.g. position, speed, spiking, LFP) to have associated valid intervals, because otherwise we cannot interpret the data or even compute common metrics like rate.\n",
    "\n",
    "These classes are still protoypes. Some of the features (e.g. valid intervals on all TimeSeries) might be incorporated into NWB itself, while other aspects (e.g. Time Queries that keep track of valid intervals) could become part of our future Query Framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General\n",
    "import pynwb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Local\n",
    "from nwb_query import TimeIntervals as TI\n",
    "from nwb_query import ContinuousData, PointData, EventData, TimeIntervals, plot_PointData_multiple, plot_ContinuousData, plot_EventData, point_to_line_dist\n",
    "import fl_extension_helpers as flh\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update({'lines.solid_capstyle': 'butt'})\n",
    "mdates.rcParams.update({'date.autoformatter.microsecond': '%H:%M:%S.%f'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and analysis parameters\n",
    "We will analyze data for one day of data for one animal, focusing particularly on a single epoch of behavioral data and a single clustered unit for spiking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what data to analyze\n",
    "d = {}\n",
    "d['anim'] = 'bon'\n",
    "d['day'] = 4 # 1-indexed\n",
    "d['epoch'] = 4 # 1-indexed\n",
    "d['cluster_id'] = 30\n",
    "\n",
    "# analysis configuration\n",
    "c = {}\n",
    "c['speed_threshold'] = 0.05 # m/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read NWB file\n",
    "For a walk-through of how to create an NWB file from data in the Frank Lab's old Matlab \"Filter Framework\" format, see the notebook 'create_franklab_nwbfile.ipynb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animday = '{}{:02d}'.format(d['anim'], d['day'])\n",
    "nwb_filename = './' + animday + '.nwb'\n",
    "\n",
    "io = pynwb.NWBHDF5IO(nwb_filename, mode='r')\n",
    "nwbf = io.read()\n",
    "sst = nwbf.session_start_time.timestamp()\n",
    "\n",
    "\n",
    "print('Successfully loaded %s' % nwb_filename)\n",
    "print(nwbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: speed of the animal across all epochs\n",
    "#### NWBFile (one animal, day) --> ContinuousData (speed)\n",
    "\n",
    "We first query for the animal's speed data across all epochs. \n",
    "\n",
    "We use these data to create a <i>ContinuousData</i> object, which is designed for data that are, in principle, continuous functions that have a value at all time points. For example, the animal has a speed at all times, but we only sample the animal's speed at some rate (e.g. our video framerate). The speed samples represent a model of the underlying continuous function.\n",
    "\n",
    "Why do we convert to a new object like ContinuousData instead of just use the existing PyNWB API? Throughout this notebook, we are prototyping a Query Framework, which would allow us to easily slice and dice our data. Importantly, this Query Framework should keeping track of the so-called \"valid intervals\": the time ranges over which our data are valid. As such, all of our time-based data will be required to have associated valid intervals. Here, the valid intervals are defined by the start and stop times of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the start/stop times of each epoch. These are the \"valid intervals\" of our behavioral data.\n",
    "epoch_intervals = np.column_stack((nwbf.epochs['start_time'][:], \n",
    "                                   nwbf.epochs['stop_time'][:]))\n",
    "\n",
    "\n",
    "# Get the speed TimeSeries object from PyNWB.\n",
    "speed_timeseries = nwbf.modules['Behavior']['Speed']['time_series']\n",
    "\n",
    "\n",
    "# Build a new ContinuousData object.\n",
    "speed_all_epochs = ContinuousData(samples=pd.DataFrame(data=speed_timeseries.data[()], columns=['speed']), \n",
    "                                  sample_times=speed_timeseries.timestamps[()], \n",
    "                                  valid_intervals=epoch_intervals)\n",
    "\n",
    "print(\"Here, the valid intervals over which we have speed data are defined by the epoch start/stop times:\\n\")\n",
    "print(speed_all_epochs.valid_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: animal speed in one epoch\n",
    "#### ContinuousData (speed), epoch --> ContinuousData (speed in one epoch)\n",
    "\n",
    "Next, we query for the speed of the animal during a single epoch by executing a <i>time query</i>. A time query simply selects a subset of the speed data occurring within some time interval (or set of intervals). Here, we select the speed data within the start/stop times of a single epoch.\n",
    "\n",
    "We automatically handle the valid intervals, so that the new ContinuousData object (speed in one epoch) will have the appropriate valid intervals. Namely, it will be the intersection of the original valid intervals (the epoch start/stop times) and the time interval we queried for (the start/stop of the epoch of interest). In this case, the valid interval of our time query perfectly overlaps one of the epochs in the original data, but this need not be the case. The resulting data will never include invalid times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start/stop of the epoch\n",
    "epoch_start_stop = np.array([nwbf.epochs['start_time'][d['epoch']-1], \n",
    "                             nwbf.epochs['stop_time'][d['epoch']-1]])\n",
    "\n",
    "# Time query to select the epoch\n",
    "speed = speed_all_epochs.time_query(epoch_start_stop)\n",
    "\n",
    "\n",
    "print(\"Doing a time query for epoch %s...\" % d['epoch'])\n",
    "print(\"Resuls in a ContinuousData object (speed in this epoch) that has the correct valid intervals for this epoch:\\n\")\n",
    "print(str(speed.valid_intervals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query for position of the animal, followed by Time Query for position in a single epoch\n",
    "#### 1. NWBFile (one animal, day) --> ContinuousData (position)\n",
    "#### 2. ContinuousData (position), epoch --> ContinuousData (position in one epoch)\n",
    "\n",
    "Just as we did for speed, we now build a ContinuousData object for the animal's position across all epochs. Again, this is because position is a continuous function (i.e. the animal has a position at all time points), but we only sample this continuous function at some sample times.\n",
    "\n",
    "Then, as with speed, we execute a time query to get the animal's position during a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 1. Dataset Query for the animal's position across all epochs, and create a ContinuousData object.\n",
    "# Note that position shares the same epoch start/stop times (i.e. same valid intervals) as speed.\n",
    "# --------------------\n",
    "pos_timeseries = nwbf.modules['Behavior']['Position']['time_series']\n",
    "pos_all_epochs = ContinuousData(samples=pd.DataFrame(data=pos_timeseries.data[()], columns=['x', 'y']), \n",
    "                                sample_times=pos_timeseries.timestamps[()], \n",
    "                                valid_intervals=epoch_intervals)\n",
    "\n",
    "# --------------------\n",
    "# 2. Time Query selecting the animal's position only during a single epoch\n",
    "# Note that we are querying for the same epoch as we did for speed.\n",
    "# --------------------\n",
    "epoch_start_stop = np.array([nwbf.epochs['start_time'][d['epoch']-1], \n",
    "                             nwbf.epochs['stop_time'][d['epoch']-1]])\n",
    "position = pos_all_epochs.time_query(epoch_start_stop)\n",
    "\n",
    "\n",
    "\n",
    "print('*** All position records for epoch ***')\n",
    "print('# of measurements = %d' % position.samples.shape[0])\n",
    "print('# of intervals = %d' % len(position.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(position.valid_intervals.durations()))\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,3))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "ax1.set_title('Position of animal %s, day %s, epoch %s' % (d['anim'], d['day'], d['epoch']))\n",
    "ax1.set_ylabel('meters')\n",
    "plot_ContinuousData(position)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: spiking of a given animal and cluster\n",
    "#### NWBFile (one animal), cluster --> PointData (spiking)\n",
    "\n",
    "We distinguish between continuous data (e.g. speed, position, head direction) from discrete point data (e.g. spiking, nose pokes) even though these are both frequently represented as \"time series\" data. This is because our interpretation of these data types--\"continuous data\" vs. \"point data\"--are fundamentally different. For continuous data, samples represent a model of the underlying continuous function, so it makes sense to, for example, interpolate between samples. In contrast, for discrete point data, samples are better though of as events generated by some underlying \"point process\", or as a series of delta functions.\n",
    "\n",
    "Now we will query for the spiking data of one clustered unit. These data are found in 'units', which is a table with a row for each clustered unit. The spike times live in the column called 'spike_times'. We also grab the 'obs_intervals' column, which contain the intervals over which we were recording from the cell, and thus when it was even possible to see a spike. \n",
    "\n",
    "Observation intervals are essential for interpreting these data, because we can only compute metrics like spike rate with respect to the time over which it was possible for us to see a spike. As we start slicing time windows of a spiking record, it can quickly become messy to keep track of these observation intervals (often requiring intersections of many different intervals). Thus, we propose that all time-based data in the Query Framework will include observation intervals.\n",
    "\n",
    "Unlike speed and position, unit spiking is not a continuous function, but instead can be thought of as a \"point process\" or a series of delta functions. We represent the unit spiking data as a PointData object, being sure to also provide the observation intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_t = nwbf.units['spike_times'][d['cluster_id']]\n",
    "valid_intervals = TimeIntervals(nwbf.units['obs_intervals'][d['cluster_id']])\n",
    "spiking_all = PointData(point_times=spikes_t, valid_intervals=valid_intervals)\n",
    "\n",
    "print('*** Spiking for cluster %s ***' % d['cluster_id'])\n",
    "print('# of spikes = %d' % spiking_all.point_times.shape[0])\n",
    "print('# of intervals = %d' % len(spiking_all.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking_all.valid_intervals.durations()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: Find spiking within all epochs where the rat did the W-alternation task\n",
    "#### PointData (spiking), TimeIntervals (epochs) --> PointData (spiking)\n",
    "\n",
    "The 'epochs' field of the NWBFile is a table with a row for each epoch. This table contains the start and stop times of each epoch, as well as the task that the animal was doing during that epoch. \n",
    "\n",
    "Here, we identify the epochs in which the animal was conducting a behavioral task called 'W-alternation'. We use these time intervals to query the spiking PointData, resulting in a new PointData object containing only the spiking data in W-alternation epochs. The valid intervals of this new PointData object are the intersection of the valid_intervals for all spiking data and the time intervals defining the W-alternation epochs. (see plot below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get start/stop times for all epochs where the rat ran the W Alternation task\n",
    "epoch_types = np.array([task.name for task in nwbf.epochs['task'][:]])\n",
    "behav_epoch_indices = np.where(epoch_types=='W-Alternation')[0]\n",
    "\n",
    "# Build a TimeIntervals query with start/stop times of W Alternation behavioral epochs\n",
    "behav_intervals = []\n",
    "for i in behav_epoch_indices:\n",
    "    epoch_start = nwbf.epochs['start_time'][i]\n",
    "    epoch_stop = nwbf.epochs['stop_time'][i]\n",
    "    behav_intervals.append([epoch_start, epoch_stop])\n",
    "behav_intervals = TimeIntervals(np.array(behav_intervals)) # convert to TimeIntervals for query\n",
    "\n",
    "# time_query on spiking during W Alternation behavioral epochs\n",
    "spiking_behav = spiking_all.time_query(behav_intervals)\n",
    "\n",
    "print('*** Spiking for cluster %s during W-Alternation behavioral epochs ***' % d['cluster_id'])\n",
    "print('# of spikes = %d' % spiking_behav.point_times.shape[0])\n",
    "print('# of intervals = %d' % len(spiking_behav.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking_behav.valid_intervals.durations()))\n",
    "\n",
    "# Plot spiking\n",
    "spikeplots = [(spiking_all, 'All spiking'),\n",
    "              (spiking_behav, 'Spikes in\\nW-Alternation epochs')]\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,3))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "\n",
    "pass # suppress output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Query: Find spiking within a single behavioral epoch\n",
    "#### PointData (spiking), TimeIntervals (epoch) --> PointData (spiking)\n",
    "\n",
    "Now we want to query for spiking in a single epoch of interest. We get the start and stop times of this epoch from the 'epochs' table, and then we use this time interval to query the spiking PointData. This results in a new PointData with just the spiking and valid intervals from that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epoch start/stop times from the NWB file\n",
    "epoch_start = nwbf.epochs['start_time'][d['epoch']-1]\n",
    "epoch_end = nwbf.epochs['stop_time'][d['epoch']-1]\n",
    "epoch_interval = TimeIntervals(np.array([epoch_start, epoch_end]))\n",
    "\n",
    "# time_query on spiking during the epoch\n",
    "spiking = spiking_all.time_query(epoch_interval)\n",
    "\n",
    "print('*** Spiking for cluster %s during epoch %d ***' % (d['cluster_id'], d['epoch']))\n",
    "print('# of spikes = %d' % spiking.point_times.shape[0])\n",
    "print('# of intervals = %d' % len(spiking.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking.valid_intervals.durations()))\n",
    "\n",
    "\n",
    "# Plot spiking\n",
    "spikeplots = [(spiking_all, 'All spiking'),\n",
    "              (spiking_behav, 'Spikes in\\nnon-sleep epochs'),\n",
    "              (spiking, 'Spikes in\\nepoch %s' % d['epoch'])]\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,3))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple(spikeplots, axis=ax1)\n",
    "\n",
    "fig2 = plt.figure(2, figsize=(15,2))\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple([spikeplots[2]], axis=ax2)\n",
    "ax2.set_title(\"Zooming in...\")\n",
    "\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Find time intervals where speed > threshold\n",
    "#### ContinuousData (speed), lambda function --> EventData (time periods where animal speed > threshold)\n",
    "Next, we want to find the time intervals within an epoch during which the animal was running faster than some speed threshold. \n",
    "\n",
    "Conceptually, we view this as an _analysis_, not a query like the time queries for epochs shown above. This is because we are not simply selecting a direct subset of a given data. Rather, we are asking for the set of time intervals when the speed data fulfills a function given boolean labmda function. The lambda function could have been something different, like \"find the times of all upward threshold crossings, and then pad this by 5 seconds on either side.\" We only mention this to clarify our terminology when we speak of queries and analyses.\n",
    "\n",
    "The resulting set of time intervals is neither a continuous functions (ContinuousData) or a discrete point processes (PointData). It is somewhat similar to a PointData, but instead of delta functions, we have a set of time intervals. As this is also a common form of data in an analysis workflow, we represent these with an EventData class. In the plot below, each vertical bar is actually a time interval where the animal was running, not a delta function like spiking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_threshold_fn = lambda x: x > c['speed_threshold']\n",
    "speed_events = speed.filter_intervals(speed_threshold_fn)\n",
    "\n",
    "print('*** Times where speed > threshold ***')\n",
    "print('# of events = %d' % len(speed_events.event_intervals))\n",
    "print('duration of events = %0.2f s' % np.sum(speed_events.durations()))\n",
    "print('# of valid intervals = %d' % len(speed_events.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(speed_events.valid_durations()))\n",
    "\n",
    "# TODO: Plot continuous speed\n",
    "fig1 = plt.figure(1, figsize=(15.5,1.5))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_EventData(speed_events, axis=ax1)\n",
    "ax1.set_yticks([1])\n",
    "ax1.set_yticklabels(['Run events \\n(speed>threshold)']) # eventually use metadata from PointData object\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: spiking during time intervals where speed > threshold\n",
    "#### PointData (spiking), EventData (bouts where speed > threshold) --> PointData (spiking)\n",
    "Now we want to query for spiking data during all of the time intervals where the animal was running above threshold (i.e. during all of the grey intervals in the plot above). To do this, we run a time query on the PointData object containing all spiking, passing in the EventData intervals from above. This results in a new PointData object whose valid intervals are the intersection of the running event intervals and the original spiking valid intervals. Here you can start to see how nice it is to have the valid intervals calculation handled for you behind the scenes. Some of these intervals contain spiking! (see plot below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiking_run = spiking.time_query(speed_events)  # Use the built-in time query method on PointProcess\n",
    "\n",
    "print('*** Spiking where speed > threshold ***')\n",
    "print('# of spikes = %d' % len(spiking_run.point_times))\n",
    "print('# of intervals = %d' % len(spiking_run.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking_run.valid_intervals.durations()))\n",
    "print()\n",
    "\n",
    "# print(spiking_run)\n",
    "\n",
    "# Plot spiking\n",
    "fig1 = plt.figure(1, figsize=(15,1.5))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_PointData_multiple([(spiking_run, 'Spiking during\\nrun events')], axis=ax1)\n",
    "\n",
    "pass # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Mark each spike with the animal's current position\n",
    "#### PointData (spiking), ContinuousData (position [m x 2]) --> PointData with marks (spike times with associated positions)\n",
    "\n",
    "Now that we have spiking during run periods, we want to associate each of these spike times with the animal's position at the time of the spike. This will be essential in building place fields for units. The process of associating each spike with associated position data can be thought of as \"marking\" the spike times with the position data. \n",
    "\n",
    "Since this is a common analysis step, we provide common functionality for marking PointData (e.g. spiking) with a ContinuousData (e.g. position). This method will automatically interpolate the ContinuousData function at the time of each spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiking_run_mark_pos = spiking_run.mark_with_ContinuousData(position)\n",
    "\n",
    "print('*** Spiking where speed > threshold, marked with position ***')\n",
    "print('# of marked spikes = %d' % len(spiking_run_mark_pos.point_times))\n",
    "print('# of intervals = %d' % len(spiking_run_mark_pos.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(spiking_run_mark_pos.valid_intervals.durations()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: Get animal locations during running events\n",
    "#### ContinuousData (position [m x 2]), EventData (bouts where speed > threshold) --> ContinuousData (position [m_new x 2])\n",
    "\n",
    "We are also interested in analyzing the animal's position during running periods.  We do a time query on the animal's position, querying for the positions during any of the set of time intervals when the animal was running. As before, the valid intervals are handled automatically for us behind the scenes, in this case intersecting the many running intervals with the position data.\n",
    "\n",
    "The plot below shows the steps of theis process: \n",
    "1. the animal's x/y position (ContinuousData)\n",
    "2. the animal's speed (ContinuousData)\n",
    "3. the time intervals when speed > threshold (Event Data)\n",
    "4. x/y position during running intervals (time query on ContinuousData with EventData input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_run = position.time_query(speed_events)\n",
    "\n",
    "print('*** Position where speed > threshold ***')\n",
    "print('# of samples = %d' % position_run.samples.shape[0])\n",
    "print('# of intervals = %d' % len(position_run.valid_intervals))\n",
    "print('duration of intervals = %0.2f s' % np.sum(position_run.valid_intervals.durations()))\n",
    "print()\n",
    "\n",
    "fig1 = plt.figure(1, figsize=(15,8))\n",
    "ax1 = fig1.add_subplot(4, 1, 1)\n",
    "ax1.set_title('Position (m)', fontsize=15)\n",
    "plot_ContinuousData(position, axis=ax1)\n",
    "ax1.label_outer()\n",
    "\n",
    "ax2 = fig1.add_subplot(4, 1, 2, sharex=ax1)\n",
    "ax2.set_title('Speed (m/s)', fontsize=15)\n",
    "plot_ContinuousData(speed, axis=ax2)\n",
    "ax2.set_xlabel('')\n",
    "ax2.label_outer()\n",
    "\n",
    "ax3 = fig1.add_subplot(4, 1, 3, sharex=ax1)\n",
    "ax3.set_title('Intervals where speed > threshold', fontsize=15)\n",
    "plot_EventData(speed_events, axis=ax3)\n",
    "ax3.label_outer()\n",
    "\n",
    "ax4 = fig1.add_subplot(4, 1, 4, sharex=ax1)\n",
    "ax4.set_title('Position (m) during intervals where speed > threshold ', fontsize=15)\n",
    "plot_ContinuousData(position_run, axis=ax4)\n",
    "# ax1.set_xlim(1136409500, 1136409600)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot spikes by location\n",
    "\n",
    "We now plot three pieces of information:\n",
    "1. rat position during all times (ContinuousData)\n",
    "2. rat position during running (ContinuousData, result of time query on running intervals)\n",
    "3. rat position during spike times (PointData marked with position: plot the marks for each spike time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1, figsize=(15,15))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "ax1.axis('equal')\n",
    "\n",
    "\n",
    "task = nwbf.epochs['task'][d['epoch']-1]\n",
    "appar = nwbf.epochs['apparatus'][d['epoch']-1]\n",
    "\n",
    "# Line showing the animal's movement during the entire epoch\n",
    "plt.plot(position.samples['x'], position.samples['y'], marker='', color='gray', label='Rat location', zorder=1)\n",
    "\n",
    "# Lines showing animal movement during each interval where it was running\n",
    "run_label = 'Rat location during movement'\n",
    "for ivl in position_run.valid_intervals:\n",
    "    ivl_data = position.time_query(TimeIntervals(ivl)).samples  # position at this valid interval \n",
    "    plt.plot(ivl_data['x'], ivl_data['y'], marker='', color='red', label=run_label, zorder=2)\n",
    "    run_label = '_' # omit later lines from legend\n",
    "\n",
    "# Markers for locations where unit spiked during animal running\n",
    "plt.scatter(spiking_run_mark_pos.marks['x'], spiking_run_mark_pos.marks['y'], marker='D', s=50, label='Location at spike times during movement', zorder=3)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('X position (m)')\n",
    "ax1.set_ylabel('Y position (m)')\n",
    "ax1.set_title('Spike-position map for {} d{} e{} c{}, speed > {:0.1f} cm/s\\ntask: {}, apparatus: {}'.format(\n",
    "    d['anim'], d['day'], d['epoch'], d['cluster_id'], c['speed_threshold'] * 100, task.name, appar.name))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Query: Task and Apparatus\n",
    "\n",
    "We also want to know some metadata about this epoch, including \n",
    "1. the Task that the animal was doing (e.g. W-alternation, Exploration, Sleep)\n",
    "2. the Apparatus that the animal was on (e.g. W-track A, Sleep box, Linear track)\n",
    "\n",
    "These are found in the 'epochs' table in the 'task' and 'apparatus' columns, respectively. The Task and Apparatus objects are both part of the Frank Lab NWB extensions. Task just contains a name and description of the task. Apparatus is a bit more complicated; it is a graph (nodes and edges) representing the topology/connectivity of the apparatus. The nodes in the graph contains coordinates that specify the geometry of the apparatus.\n",
    "\n",
    "For details on how these objects are generated, see create_franklab_nwbfile.ipynb. For details on the Frank Lab NWB extensions, see fl_extension.py (for the available classes), create_franklab_spec.ipynb (for creation of the YAML spec), and franklab.extensions.yaml (for the spec itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for task\n",
    "# NWBFile (one animal, day), epoch --> franklab_apparatus.Task\n",
    "task = nwbf.epochs['task'][d['epoch']-1]\n",
    "\n",
    "# Query for apparatus\n",
    "# NWBFile (one animal, day), epoch --> franklab_apparatus.Apparatus\n",
    "appar = nwbf.epochs['apparatus'][d['epoch']-1]\n",
    "\n",
    "print('The task is just a name and description:')\n",
    "print(task)\n",
    "print('\\n')\n",
    "print('The apparatus is a graph (nodes and edges) representing topology/connectivity and geometry:')\n",
    "print(appar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2D position-normalized spiking\n",
    "\n",
    "Now we have all the pieces necessary to plot the 2D position-normalized spiking for this unit. Below, we plot the steps of this process:\n",
    "1. time (sec) the animal spent at different locations\n",
    "2. number of spikes (spike count) when the animal was at differnt locations\n",
    "3. position-normalized spike rate (spikes per sec that the animal was in a given location)\n",
    "\n",
    "We can see that the position-normalized spike rate (fig.3) reflects the spike-position figure from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "padding = 0.1\n",
    "nbins = 20\n",
    "minx = np.min(position.samples['x'])\n",
    "maxx = np.max(position.samples['x'])\n",
    "miny = np.min(position.samples['y'])\n",
    "maxy = np.max(position.samples['y'])\n",
    "binsx = np.linspace(minx-padding, maxx+padding, nbins)\n",
    "binsy = np.linspace(miny-padding, maxy+padding, nbins)\n",
    "\n",
    "# Compute histogram of position during entire epoch\n",
    "fig1 = plt.figure(figsize=(15,15))\n",
    "ax1 = plt.subplot(111)\n",
    "plt.title('Animal position')\n",
    "plt.xlabel('X position (m)')\n",
    "plt.ylabel('Y position (m)')\n",
    "n_pos, xbins_pos, ybins_pos, img_pos = plt.hist2d(position.samples['x'], position.samples['y'], bins=[binsx, binsy], cmap='Reds')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Time spent (s)')\n",
    "flh.plot_fl_appar_geom(appar, ax1, label_nodes=True) # Plot apparatus geometry\n",
    "\n",
    "\n",
    "# Compute histogram of position during spiking\n",
    "fig2 = plt.figure(figsize=(15,15))\n",
    "ax2 = plt.subplot(111)\n",
    "plt.title('Spike-position')\n",
    "plt.xlabel('X position (m)')\n",
    "plt.ylabel('Y position (m)')\n",
    "n_spikes, xbins_spikes, ybins_spikes, img_pos = plt.hist2d(spiking_run_mark_pos.marks['x'], spiking_run_mark_pos.marks['y'], bins=[binsx, binsy], cmap='Reds')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Number of spikes')\n",
    "flh.plot_fl_appar_geom(appar, ax2, label_nodes=True) # Plot apparatus geometry\n",
    "\n",
    "\n",
    "# Compute position normalized spiking\n",
    "n_pos = np.add(n_pos, 0.0001)\n",
    "norm_spikes = np.divide(n_spikes, n_pos)\n",
    "\n",
    "# Plot position normalized spiking\n",
    "fig3 = plt.figure(figsize=fig2.get_size_inches())\n",
    "ax3 = plt.subplot(111)\n",
    "plt.title('Position normalized spiking\\nfor {} d{} e{} c{}, speed > {:0.1f} cm/s\\ntask: {}, apparatus: {}'.format(\n",
    "    d['anim'], d['day'], d['epoch'], d['cluster_id'], c['speed_threshold'] * 100, task.name, appar.name))\n",
    "plt.xlabel('X position (m)')\n",
    "plt.ylabel('Y position (m)')\n",
    "plt.imshow(norm_spikes.T, origin='lower', aspect='auto', cmap='Reds', extent=(minx, maxx, miny, maxy))\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Position-spike rate (spikes / s)')\n",
    "flh.plot_fl_appar_geom(appar, ax3, label_nodes=True) # Plot apparatus geometry\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for animal positions during sleep epochs, and plot on top of the sleep box geometry\n",
    "We use the 'epochs' table to find which were sleep epochs. We then get the animal's position during all of these epochs, storing each in a ContinuousData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of all the sleep epochs\n",
    "task_names = np.array([task.name for task in nwbf.epochs['task'][:]])\n",
    "sleep_epochs = np.where(task_names == 'Sleep')[0]\n",
    "\n",
    "# Plot position of the animal during each sleep epoch\n",
    "# Using time queries for each sleep epoch\n",
    "plt.figure()\n",
    "plt.title('Position during sleep epochs on day {}'.format(d['day']))\n",
    "plt.xlabel('X position (m)')\n",
    "plt.ylabel('Y position (m)')\n",
    "ax = plt.subplot(111)\n",
    "for epoch_idx in sleep_epochs:\n",
    "    epoch_query = TimeIntervals(epoch_intervals[epoch_idx])\n",
    "    epoch_pos = pos_all_epochs.time_query(epoch_query)\n",
    "    plt.plot(epoch_pos.samples['x'], epoch_pos.samples['y'], marker='', label='e{}'.format(epoch_idx+1))\n",
    "\n",
    "# Get the Sleep Box apparatus (Same apparatus for all sleep epochs, so just use the first)\n",
    "first_sleep_epoch = sleep_epochs[0]\n",
    "appar = nwbf.epochs['apparatus'][first_sleep_epoch]\n",
    "\n",
    "# Convert the Frank Lab apparatus to a Network X graph and plot it\n",
    "H = nx.Graph(name='sleep box')\n",
    "for n in appar.nodes.values():\n",
    "    flh.add_fl_node_to_nx_graph(n, H)\n",
    "flh.plot_nx_appar_geom(H, ax)\n",
    "ax.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the file io\n",
    "io.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

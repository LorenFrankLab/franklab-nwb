{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an NWB 2.0 File with Frank Lab data\n",
    "\n",
    "This notebook shows how to create an [Neurodata Without Borders: Neurophysiology (NWB:N)](https://neurodatawithoutborders.github.io/) file from publicly available Frank Lab data. We will create an NWB file storing one day of data from one animal, inlcluding both awake behaving and sleep epochs, using the [PyNWB](https://pynwb.readthedocs.io/en/latest/) API. You can think of the PyNWB API as a toolbox for working with NWB files in the Python programming language.\n",
    "\n",
    "We will use the [hc-6 dataset](https://crcns.org/data-sets/hc/hc-6/about-hc-5), collected by Dr. Mattias Karlsson and Dr. Margaret Carr working in the lab of Dr. Loren Frank. While these data are publicly available, please read the section titled \"Conditions for using the data\" on CRCNS. After setting up an account with CRCNS, you can download the data via the link in the section titled \"How to download data.\" Each animal's compressed data will be around 3 GB. \n",
    "\n",
    "Instructions:\n",
    "1. If you don't already have an account with CRCNS, create one.\n",
    "2. Download the data for animal Bon from the CRCNS [hc-6 dataset](https://crcns.org/data-sets/hc/hc-6/about-hc-5) and uncompress it. You should see a directory called \"Bon\".\n",
    "3. In the second cell of this notebook (after importing dependencies), update the \"data_dir\" variable to the full path to your \"Bon\" directory.\n",
    "4. Run each cell in order. We recommend reading the cell header comments and waiting for each cell to complete to read the outputs along the way.\n",
    "\n",
    "Note: If you encounter any errors, the first thing to try is to restart the kernel and re-run the cells. \n",
    "\n",
    "After completing this notebook, you might be interested in the 'place_field_with_queries.ipynb' notebook for a demonstration / sneak peak of how Frank Lab NWB files can be effectively queried to run a cononical place field analysis. That notebook introduces a set of useful query classes, which are still in bata phase but could become incorporated into a future Query Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pynwb\n",
    "\n",
    "# General dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Time\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "# Helpers for parsing Frank Lab Matlab data\n",
    "import nspike_helpers as ns \n",
    "import query_helpers as qu\n",
    "\n",
    "# Frank Lab PyNWB extensions and extension-related helpers\n",
    "import fl_extension as fle\n",
    "import fl_extension_helpers as flh\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "mdates.rcParams.update({'date.autoformatter.microsecond': '%H:%M:%S.%f'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which data do you want to load?\n",
    "\n",
    "We will create an NWB file storing data from one experimental day for one animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "# ~ ~ ~ ~ UPDATE THIS VARIABLE WITH THE PATH TO YOUR DATA ~ ~ ~ ~\n",
    "# -------\n",
    "# Path to your data directory (e.g. the directory called \"Bon\" downloaded from CRCNS)\n",
    "data_dir = os.path.expanduser('~/Data/Bon')  \n",
    "\n",
    "# -------\n",
    "# You don't need to change these unless you are trying out other days or animals.\n",
    "# We recommend starting with animal 'Bon', day 4.\n",
    "# -------\n",
    "day = 4\n",
    "animal = 'Bon'\n",
    "dataset_zero_time = datetime(2006, 1, day, 12, 0, 0, tzinfo=tz.gettz('US/Pacific'))   # Time the experiment was conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the new PyNWB object\n",
    "\n",
    "The PyNWB object provides an API (i.e. set of easily-used functions) that allow us to store our data in the\n",
    "appropriate places for eventually saving as a valid NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbf = pynwb.NWBFile(\n",
    "           session_description='Frank Lab CRCNS data for animal {0}, day {1}'.format(animal, day),\n",
    "           identifier='{0}{1:04}'.format(animal, day),\n",
    "           session_start_time=dataset_zero_time,\n",
    "           file_create_date=datetime.now(tz.tzlocal()),\n",
    "           lab='Frank Laboratory',\n",
    "           experimenter='Mattias Karlsson',\n",
    "           institution='UCSF',\n",
    "           experiment_description='Recordings from awake behaving rat',\n",
    "           session_id=data_dir)\n",
    "\n",
    "print(\"Here is the top-level structure of our new (still mostly empty) PyNWB object:\")\n",
    "print(nwbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PyNWB objects for storing behavioral timeseries data\n",
    "\n",
    "PyNWB provides several datatypes for specific kinds of behavior data. This allows anyone using PyNWB to know what kinds of data are stored in which places in the PyNWB file. However, all of these are examples of timeseries data (i.e. data with assocaited timestamps). Here, we use the following:\n",
    "- spatial position (x/y) will be stored in a [Position](https://pynwb.readthedocs.io/en/latest/pynwb.behavior.html#pynwb.behavior.Position) object\n",
    "- head direction (angle) will be stored in a [CompassDirection](https://pynwb.readthedocs.io/en/latest/pynwb.behavior.html#pynwb.behavior.CompassDirection) object\n",
    "- speed (m/s) will be stored in a more general [BehavioralTimeSeries](https://pynwb.readthedocs.io/en/latest/pynwb.behavior.html#pynwb.behavior.BehavioralTimeSeries) object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = pynwb.behavior.Position(name='Position')\n",
    "head_dir = pynwb.behavior.CompassDirection(name='Head Direction')\n",
    "speed = pynwb.behavior.BehavioralTimeSeries(name='Speed')\n",
    "\n",
    "print(\"Here are the PyNWB objects where we will store behavior data:\")\n",
    "print(position)\n",
    "print(head_dir)\n",
    "print(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse animal behavior data\n",
    "Next, we parse the behavioral data from the old Frank Lab Matlab files and them in the PyNWB datatypes we initialized above.\n",
    "We concatenate the data across epochs into a single big timeseries (i.e. there are no formal separations between epochs). Accessing particular epochs is done via time queries on the epoch start/stop times (see place_field_with_queries.ipynb).\n",
    "\n",
    "We add the animal's 2D spatial position by calling the [create_spatial_series](https://pynwb.readthedocs.io/en/latest/pynwb.behavior.html?highlight=create_spatial_series#pynwb.behavior.Position.create_spatial_series) method on the \n",
    "PyNWB Position object. Similarly, for speed we use the [create_timeseries](https://pynwb.readthedocs.io/en/latest/pynwb.behavior.html?highlight=create_spatial_series#pynwb.behavior.BehavioralTimeSeries.create_timeseries) method on the PyNWB BehavioralTimeSeries object\n",
    "to add the animal's speed for this epoch and day. The differences between these two are minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_data = ns.parse_franklab_behavior_data(data_dir, animal, day)\n",
    "epoch_time_ivls = []\n",
    "time_idx, x_idx, y_idx, dir_idx, vel_idx = range(5)  # column ordering of the behavioral data matrix\n",
    "\n",
    "\n",
    "# Accumulate behavior samples across all epochs\n",
    "pos_samples = np.zeros((0, 2)) # x/y positions (n x 2)\n",
    "dir_samples = np.array([])\n",
    "speed_samples = np.array([])\n",
    "behavior_timestamps = []  # behavior timestamps are shared\n",
    "\n",
    "# Loop over each epoch of this day\n",
    "for epoch_num, epoch_data in behavior_data.items():\n",
    "    m_per_pixel = epoch_data['cmperpixel'][0,0]/100    # meters / pixel conversion factor\n",
    "    \n",
    "    # Behavior samples for this epoch (position, head direction, speed)\n",
    "    epoch_pos_samples = epoch_data['data'][:, (x_idx, y_idx)] * m_per_pixel\n",
    "    epoch_dir_samples = epoch_data['data'][:, dir_idx]\n",
    "    epoch_speed_samples = epoch_data['data'][:, vel_idx] * m_per_pixel\n",
    "    \n",
    "    # Timestamps for this epoch (note that we convert timestamps to POSIX time)\n",
    "    epoch_timestamps = epoch_data['data'][:, time_idx] + dataset_zero_time.timestamp()\n",
    "    \n",
    "    # Concatenate this epoch's data to the across-epoch lists\n",
    "    pos_samples = np.concatenate((pos_samples, epoch_pos_samples), axis=0)\n",
    "    dir_samples = np.concatenate((dir_samples, epoch_dir_samples), axis=0)\n",
    "    speed_samples = np.concatenate((speed_samples, epoch_speed_samples), axis=0)\n",
    "    behavior_timestamps = np.concatenate((behavior_timestamps, epoch_timestamps), axis=0)\n",
    "    \n",
    "    # Store the times of epoch start and end. We will use these later to build the 'epochs' table\n",
    "    epoch_time_ivls.append([epoch_timestamps[0], epoch_timestamps[-1]])\n",
    "    \n",
    "\n",
    "# Add the across-epochs behavioral data to the PyNWB objects\n",
    "# See place_field_with_queries.ipynb for examples of how we query these for specific epochs\n",
    "position.create_spatial_series(name='Position', \n",
    "                               timestamps=behavior_timestamps, \n",
    "                               data=pos_samples, \n",
    "                               reference_frame='corner of video frame')\n",
    "head_dir.create_spatial_series(name='Head direction', \n",
    "                               timestamps=behavior_timestamps, \n",
    "                               data=dir_samples, \n",
    "                               reference_frame='0=direction of top of video frame; ' + \n",
    "                                   'positive values clockwise (need to confirm this)')\n",
    "speed.create_timeseries(name='Speed', \n",
    "                        timestamps=behavior_timestamps, \n",
    "                        data=speed_samples, \n",
    "                        unit='m/s', \n",
    "                        description='smoothed movement speed estimate')\n",
    "\n",
    "    \n",
    "print(\"Here are the same PyNWB objects, now filled with behavior data from each epoch:\")\n",
    "print(position)\n",
    "print(head_dir)\n",
    "print(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add behavior objects to the NWBFile\n",
    "Now that we have added the behavioral data to separate PyNWB objects (position, head_dir, speed), we need to put these objects into the appropriate place in the main NWBFile object. Specifically, we will add them to a [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule), which is basically just a named bucket where we can store processed data of a particular type.  In our case, we will create a [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule) called \"behavior\" for storing processed behavior data. All of the [ProcessingModules](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule) that have been added can be found in [NWBFile.modules](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=Acquisition#pynwb.file.NWBFile.modules).\n",
    "\n",
    "We use the [add_data_interface](https://pynwb.readthedocs.io/en/latest/pynwb.core.html#pynwb.core.NWBDataInterface) method to add each of our behavior objects to the [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule). The term \"data interface\" refers to an [NWBDataInterface](https://pynwb.readthedocs.io/en/latest/pynwb.core.html#pynwb.core.NWBDataInterface), which you might come across in reading the documentation. This terminology can be quite confusing, so we will reiterate what the terms mean:\n",
    "- a [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule) is a named bucket where we put processed data of a particular type (e.g. behavior data)\n",
    "- an [NWBDataInterface](https://pynwb.readthedocs.io/en/latest/pynwb.core.html#pynwb.core.NWBDataInterface) is any datatype that we want to store in a [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule). All of the datatypes that we used to store our behavior data are examples of this.\n",
    "\n",
    "One other thing to note here is the distinction between the PyNWB NWBFile object and the NWB file that we are creating. We have not yet saved anything to an NWB file on disk. We will do that at the end, after we have added everything to the PyNWB NWBFile object in the appropriate locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ProcessingModule for behavior data\n",
    "behav_mod = nwbf.create_processing_module(name='Behavior', \n",
    "                                          description='Behavioral data')\n",
    "\n",
    "# Add the position, head direction and speed data to the ProcessingModule\n",
    "behav_mod.add_data_interface(position)\n",
    "behav_mod.add_data_interface(head_dir)\n",
    "behav_mod.add_data_interface(speed)\n",
    "\n",
    "print(\"Here is the ProcessingModule (i.e. bucket in the NWB file) where we will store all of the behavior data:\")\n",
    "print(behav_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the apparatus geometries from animal position data\n",
    "Accurately representing the geometry and topology of behavioral apparatuses (i.e. tracks, mazes, open fields, sleep boxes) is essential for interpreting the data. Here, we estimate track geometry by visually inspecting the animal position records. At the top of this cell, hard-coded values are provided for animal 'Bon', day 4. For other animals and days, you might need to adjust these values based on the plots of position data on each apparatus.  \n",
    "\n",
    "In the next cell, we will incorporate these geometry coordinates into formal Frank Lab Apparatus objects (franklab.extensions.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Hard-coded estimates of apparatus geometry for animal 'Bon', day 4.\n",
    "# For other datasets, replace these with accurate estimates from the position plots.\n",
    "# --------------\n",
    "# Sleep box polygon [lower left, upper left, upper right, lower right]\n",
    "sleep_box_polygon = {'sleep box': [[1.1, 0.7], [1.1, 1.4], [1.95, 1.4], [1.95, 0.7]]}\n",
    "\n",
    "\n",
    "# W-track A: \n",
    "# -- linearized segments represent the start/end of track segments/\"arms\"\n",
    "# -- reward well locations\n",
    "wtrackA_segments = {'left arm': [[1.4, 0.73], [1.99, 0.77]],\n",
    "                    'left_branch': [[1.99, 0.77], [1.97, 1.15]],\n",
    "                     'middle arm': [[1.38, 1.1], [1.97, 1.15]],\n",
    "                     'right_branch': [[1.97, 1.15], [1.95, 1.5]],\n",
    "                     'right arm': [[1.37, 1.5], [1.95, 1.5]]}\n",
    "wtrackA_reward_wells = {'left reward well': [1.4, 0.73],\n",
    "                        'middle reward well': [1.38, 1.1],\n",
    "                        'right reward well': [1.37, 1.5]}\n",
    "\n",
    "# W-track B\n",
    "# -- linearized segments start/end \n",
    "# -- reward well locations\n",
    "wtrackB_segments = {'left arm': [[0.37, 0.55], [0.35, 1.3]],\n",
    "                     'middle arm': [[0.67, 0.6], [0.64, 1.3]],\n",
    "                     'right arm': [[1.0, 0.6], [0.95, 1.3]],\n",
    "                     'left_branch': [[0.37, 0.55], [0.67, 0.6]],\n",
    "                     'right_branch': [[0.67, 0.6], [1.0, 0.6]]}\n",
    "wtrackB_reward_wells = {'left reward well': [0.35, 1.3],\n",
    "                        'middle reward well': [0.64, 1.3],\n",
    "                        'right reward well': [0.95, 1.3]}\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Find which epochs were on which apparatus (sleep box, w-track A, or w-track B)\n",
    "# --------------\n",
    "sleep_epochs, wtrackA_epochs, wtrackB_epochs = flh.separate_epochs_by_apparatus(data_dir, animal, day)\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Plot animal position on each apparatus\n",
    "# -------------\n",
    "pos_series = position['spatial_series']\n",
    "sleep_fig = flh.plot_position_by_epochs(animal, pos_series, np.array(epoch_time_ivls),\n",
    "                                        sleep_epochs, 'Sleep box epochs: animal %s, day %s' % (animal, day))\n",
    "wtrackA_fig = flh.plot_position_by_epochs(animal, pos_series, np.array(epoch_time_ivls),\n",
    "                                          wtrackA_epochs, 'W-track A epochs: animal %s, day %s' % (animal, day))\n",
    "wtrackB_fig = flh.plot_position_by_epochs(animal, pos_series, np.array(epoch_time_ivls),\n",
    "                                          wtrackB_epochs, 'W-track B epochs: animal %s, day %s' % (animal, day))\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Overlay plot of estimated apparatuses (estimated using the animal position plots)\n",
    "# -------------\n",
    "flh.overlay_apparatus_geom(sleep_fig, points={}, segments={}, polygons=sleep_box_polygon)\n",
    "flh.overlay_apparatus_geom(wtrackA_fig, points=wtrackA_reward_wells, segments=wtrackA_segments, polygons={})\n",
    "flh.overlay_apparatus_geom(wtrackB_fig, points=wtrackB_reward_wells, segments=wtrackB_segments, polygons={})\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store apparatuses as NWB Apparatus objects (Frank Lab extension)\n",
    "\n",
    "As shown above, we have three apparatuses: Sleep Box, W-track A, and W-track B. We represent each behavioral apparatus as a Frank Lab Apparatus (franklab.extensions.yaml), which uses a graph representation (i.e. nodes and edges) to represent the topology of a track. Each Node represents an important component of the apparatus:\n",
    "- PointNode represents 0D point with an x/y position (e.g. reward well, novel object)\n",
    "- SegmentNode represents a 1D line segment (e.g. linearized maze arm)\n",
    "- PolygonNode represents a 2D area (e.g. open field / non-linearizable area)\n",
    "\n",
    "Each Node object has a 'coords' field that describes its spatial geometry, which we found in the previous cell. Nodes sharing at least one coordinate can be represented as spatially connected by adding an Edge. For example, we can represent a reward well (PointNode) at the end of a linearized W-track arm (SegmentNode) by adding an Edge connecting those nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Apparatus 1: Sleep Box\n",
    "#   The Sleep Box consists of a single PolygonNode representing the polygon area of the box.\n",
    "#   The vertices of this polygon are defined in the \"coords\" property of the PolygonNode.\n",
    "#   There are no edges since there is only one node.\n",
    "# ---------\n",
    "sleepbox_nodes = flh.get_franklab_nodes(points={}, segments={}, polygons=sleep_box_polygon) \n",
    "sleepbox_edges = []  \n",
    "sleep_box_apparatus = fle.Apparatus(name='Sleep Box', nodes=sleepbox_nodes, edges=sleepbox_edges)\n",
    "\n",
    "# ---------\n",
    "# Apparatus 2: W-track A\n",
    "#   This apparatus consists of several SegmentNodes representing the linearized segements of the W-track\n",
    "#   and three PointNodes representing the reward wells at the end of the three \"arms\" of the track.\n",
    "#   There are Edges connecting the SegmentNodes and PointNodes according to the topology of the\n",
    "#   track (i.e. edges connecting track components that are, in fact, connected spatially).\n",
    "# ---------\n",
    "wtrack_A_nodes = flh.get_franklab_nodes(points=wtrackA_reward_wells, segments=wtrackA_segments, polygons={})\n",
    "wtrack_A_edges = flh.find_edges(wtrack_A_nodes) # finds Nodes with 1 or more shared x/y coords\n",
    "wtrack_A_apparatus = fle.Apparatus(name='W-track A', nodes=wtrack_A_nodes, edges=wtrack_A_edges)\n",
    "\n",
    "\n",
    "# ---------\n",
    "# Apparatus 3: W-track B\n",
    "#   The second W-track used in this experiment has the same basic structure as W-track A, but\n",
    "#   its geometry is distinct and thus should be represented as a separate Apparatus object.\n",
    "# ---------\n",
    "wtrack_B_nodes = flh.get_franklab_nodes(points=wtrackB_reward_wells, segments=wtrackB_segments, polygons={}) \n",
    "wtrack_B_edges = flh.find_edges(wtrack_B_nodes)\n",
    "wtrack_B_apparatus = fle.Apparatus(name='W-track B', nodes=wtrack_B_nodes, edges=wtrack_B_edges)\n",
    "\n",
    "\n",
    "print(\"Here are the three Apparatus objects. Notice that they consist of 'edges' and 'nodes'.\")\n",
    "print(sleep_box_apparatus)\n",
    "print(wtrack_A_apparatus)\n",
    "print(wtrack_B_apparatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Apparatuses in the NWBFile object\n",
    "\n",
    "After building the Apparatus objects, we store them in the \"Behavior\" [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule), just like we did with the other behavioral data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Add all three Apparatuses to the \"Behavior\" ProcessingModule\n",
    "# ---------\n",
    "behav_mod.add_data_interface(sleep_box_apparatus)\n",
    "behav_mod.add_data_interface(wtrack_A_apparatus)\n",
    "behav_mod.add_data_interface(wtrack_B_apparatus)\n",
    "\n",
    "print(\"Note that our fl_extension.Apparatus objects are in the ProcessingModule:\")\n",
    "print(behav_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent behavioral tasks using Frank Lab NWB extension\n",
    "We represent each behavioral task as a Frank Lab Task (franklab.extensions.yaml). This object simply contains a name and a detailed description of the task. For the dataset here, we only have two tasks: W-Alternation and Sleep.\n",
    "\n",
    "We then store Task objects in the \"Behavior\" [ProcessingModule](https://pynwb.readthedocs.io/en/latest/pynwb.base.html#pynwb.base.ProcessingModule), just like we did with the other behavioral data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'Sleep'\n",
    "description = 'The animal sleeps or wanders freely around a small, empty box.'\n",
    "behav_mod.add_data_interface(fle.Task(name=task_name, description=description))\n",
    "\n",
    "task_name = 'W-Alternation'\n",
    "task_description = 'The animal runs in an alternating W pattern between three neighboring arms of a maze.'\n",
    "behav_mod.add_data_interface(fle.Task(name=task_name, description=task_description))\n",
    "\n",
    "print(\"Note that we've added fl_extension.Task objects to our ProcessingModule:\")\n",
    "print(behav_mod)\n",
    "print(behav_mod['W-Alternation'])\n",
    "print(behav_mod['Sleep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store epoch metadata in the NWBFile object\n",
    "We store metadata in the top-level [NWBFile.epochs](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=epochs#pynwb.file.NWBFile.epochs) table. By default, there are required columns for 'start_time', 'stop_time', and 'tags'. We add additional metadata columns for the epoch's Task, Apparatus, etc. using the [NWBFile.add_epoch_column()](https://pynwb.readthedocs.io/en/latest/pynwb.file.html#pynwb.file.NWBFile.add_epoch_column) method. After we have all of the metadata columns set up, we add each epoch as a new row of the table using the [NWBFile.add_epoch()](https://pynwb.readthedocs.io/en/latest/pynwb.file.html#pynwb.file.NWBFile.add_epoch) method.\n",
    "\n",
    "<i>Take a look under the PyNWB hood</i>:</br>\n",
    "Each epoch occurs in a discrete time interval defined by its start and stop times. As such, the [NWBFile.epochs](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=epochs#pynwb.file.NWBFile.epochs) table is an instance of [TimeIntervals](https://pynwb.readthedocs.io/en/latest/pynwb.epoch.html?highlight=TimeIntervals#pynwb.epoch.TimeIntervals), which is itself an instance of [DynamicTable](https://pynwb.readthedocs.io/en/latest/pynwb.core.html#pynwb.core.DynamicTable). Later, we will store electrodes and clustered units in two other DynamicTables. One of the advantages of using DynamicTables is it allows for adding arbitrary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Load epochs metadata from the Frank Lab Matlab files\n",
    "# ---------\n",
    "all_epochs_metadata = ns.parse_franklab_task_data(data_dir, animal, day)\n",
    "\n",
    "# ---------\n",
    "# Add metadata columns to the NWBFile.epochs table\n",
    "# By default, ithas columns for 'start_time', 'stop_time', and 'tags'.\n",
    "# ---------\n",
    "nwbf.add_epoch_column(name='exposure', description='number of exposures to this environment')\n",
    "nwbf.add_epoch_column(name='task', description='behavioral task for this epoch')\n",
    "nwbf.add_epoch_column(name='apparatus', description='behavioral apparatus for this epoch')\n",
    "\n",
    "# ---------\n",
    "# Iteratively add each epoch to the NWBFile.epochs table\n",
    "# ---------\n",
    "for epoch_num, epoch_metadata in all_epochs_metadata.items():\n",
    "    \n",
    "    # start and stop times were inferred from the behavior data earlier\n",
    "    epoch_start_time, epoch_stop_time = epoch_time_ivls[epoch_num-1]  \n",
    "    \n",
    "    # meter per pixel ratio is also in the behavior data\n",
    "    m_per_pixel = behavior_data[epoch_num]['cmperpixel'][0,0]/100  \n",
    "    \n",
    "    # Frank Lab Task (from the \"Behavior\" ProcessingModule)\n",
    "    epoch_task = flh.get_franklab_task(epoch_metadata, behav_mod)\n",
    "    \n",
    "    # Frank Lab Apparatus (from the \"Behavior\" ProcessingModule)\n",
    "    epoch_apparatus = flh.get_franklab_apparatus(epoch_metadata, behav_mod)\n",
    "    \n",
    "    epoch_exposure_num = ns.get_exposure_num(epoch_metadata)\n",
    "    \n",
    "    # Required column 'tags'. We do not presently use this.\n",
    "    epoch_tags = ''\n",
    "    \n",
    "    # Add this epoch to the NWBFile.epochs table\n",
    "    # Note that task and apparatus are soft links to the \"Behavior\" ProcessingModule, \n",
    "    # so they will not be unnecessarily duplicated within the NWBFile\n",
    "    nwbf.add_epoch(start_time=epoch_start_time,\n",
    "                   stop_time=epoch_stop_time,\n",
    "                   exposure=epoch_exposure_num,\n",
    "                   task=epoch_task,\n",
    "                   apparatus=epoch_apparatus,\n",
    "                   tags=epoch_tags)\n",
    "\n",
    "\n",
    "print(\"Here is an example epoch from the table.\\nNote that the 'task' and 'apparatus' columns point to our Frank Lab extension objects.\\n\")\n",
    "print(nwbf.epochs.to_dataframe().iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tetrodes metadata\n",
    "\n",
    "1. Each tetrode gets an [ElectrodeGroup](https://pynwb.readthedocs.io/en/latest/pynwb.ecephys.html#pynwb.ecephys.ElectrodeGroup), where we store metadata about the tetrode such a unique name and the region of the brain it's in. \n",
    "\n",
    "2. Each individual recording channel (i.e. each of the 4 tetrode channels) gets its own row in the top-level [NWBFile.electrodes](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=epochs#pynwb.file.NWBFile.electrodes) table. Here we store metadata about the electrode such as its location/depth in the brain, and the tetrode that it is part of.  We use the [NWBFile.add_electrode()](https://pynwb.readthedocs.io/en/latest/pynwb.file.html#pynwb.file.NWBFile.add_electrode) method to add each channel. As with the NWBFile.epochs table, discussed above, this is a DynamicTable.\n",
    "\n",
    "3. Each tetrode gets an \"Electrode Table Region\", an instance of [DynamicTableRegion](https://pynwb.readthedocs.io/en/latest/pynwb.core.html?highlight=DynamicTableRegion#pynwb.core.DynamicTableRegion). This is just a slice into the [NWBFile.electrodes](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=epochs#pynwb.file.NWBFile.electrodes) table selecting the channels that go with the tetrode. We use the [create_electrode_table_region()]() method to add each tetrode's Electrode Table Region.\n",
    "\n",
    "4. Since LFP is often taken from a subset of a tetrode's channels, we create an Electrode Table Region for each tetrode's LFP channles. For example, if LFP was taken from just the first channel of a tetrode, we would create an Electrode Table Region just pointing to that channel of the [NWBFile.electrodes](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=epochs#pynwb.file.NWBFile.electrodes) table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse tetrodes metadata from the old Frank Lab Matlab files\n",
    "tetrode_metadata = ns.parse_franklab_tetrodes(data_dir, animal, day)\n",
    "\n",
    "# Represent our acquisition system with a 'Device' object\n",
    "recording_device = nwbf.create_device(name='NSpike acquisition system')\n",
    "\n",
    "# Four channels per tetrode by definition \n",
    "num_chan_per_tetrode = 4     \n",
    "\n",
    "# Initialize dictionaries to store the metadata\n",
    "tet_electrode_group = {}  # group for each tetrode\n",
    "tet_electrode_table_region = {}  # region for each tetrode (all channels)\n",
    "lfp_electrode_table_region = {}  # region for each tetrode's LFP channels\n",
    "\n",
    "chan_num = 0   # Incrementing channel number\n",
    "for tet_num, tet in tetrode_metadata.items():\n",
    "    \n",
    "    # Define some metadata parameters\n",
    "    tetrode_name = \"%02d-%02d\" % (day, tet_num) \n",
    "    impedance = np.nan\n",
    "    filtering = 'unknown - likely 600Hz-6KHz'\n",
    "    location = ns.get_franklab_tet_location(tet)  # area/subarea in the brain\n",
    "    depth = ns.get_franklab_tet_depth(tet)  # depth in the brain\n",
    "    description = \"tetrode {tet_num} located in {location} on day {day}\".format(\n",
    "        tet_num=tet_num, location=location, day=day)\n",
    "    \n",
    "    # 1. Represent the tetrode in NWB as an ElectrodeGroup\n",
    "    tet_electrode_group[tet_num] = nwbf.create_electrode_group(name=tetrode_name,\n",
    "                                                               description=description,\n",
    "                                                               location=location,\n",
    "                                                               device=recording_device)\n",
    "    \n",
    "    # 2. Represent each channels of the tetrode as a row in the NWBFile.electrodes table.\n",
    "    #    We do not have x and y coordinates for electrodes, so we set to np.nan.\n",
    "    for i in range(num_chan_per_tetrode):\n",
    "            nwbf.add_electrode(x=np.nan,  \n",
    "                               y=np.nan,\n",
    "                               z=depth,\n",
    "                               imp=impedance,\n",
    "                               location=location,\n",
    "                               filtering=filtering,\n",
    "                               group=tet_electrode_group[tet_num],  # tetrode this electrode belongs to\n",
    "                               group_name=tet_electrode_group[tet_num].name,\n",
    "                               id=chan_num)\n",
    "            chan_num = chan_num + 1  # total number of channels processed so far across all tets\n",
    "            \n",
    "    # 3. Create an Electrode Table Region (slice into the electrodes table) for each tetrode\n",
    "    table_region_description = 'tetrode %d all channels' % tet_num\n",
    "    table_region_name = '%d' % tet_num\n",
    "    table_region_rows = list(range(chan_num - num_chan_per_tetrode, chan_num)) # rows of NWBFile.electrodes table\n",
    "    tet_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=table_region_rows,\n",
    "        description=table_region_description,\n",
    "        name=table_region_name)\n",
    "\n",
    "    # 4. Create an ElectrodeTableRegion for each tetrode's LFP recordings\n",
    "    lfp_channels = [chan_num - num_chan_per_tetrode] # Assume that LFP is taken from the first channel\n",
    "    table_region_description = 'tetrode %d LFP channels' % tet_num\n",
    "    lfp_electrode_table_region[tet_num] = nwbf.create_electrode_table_region(\n",
    "        region=lfp_channels,\n",
    "        description=table_region_description,\n",
    "        name='electrodes')\n",
    "\n",
    "print(\"Here is an example ElectrodeGroup for a tetrode: \")\n",
    "print(tet_electrode_group[1])\n",
    "print('\\n')\n",
    "print(\"Here is the ElectrodeTableRegion for that tetrode:\")\n",
    "print(tet_electrode_table_region[1])\n",
    "print('\\n')\n",
    "print(\"Here is an example channel from the NWBFile.electrodes table:\")\n",
    "print(nwbf.electrodes.to_dataframe().iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process LFP\n",
    "\n",
    "We store LFP data from all of our tetrodes in a single [LFP](https://pynwb.readthedocs.io/en/latest/pynwb.ecephys.html?highlight=LFP#pynwb.ecephys.LFP) object. We use the [LFP.create_electrical_series()](https://pynwb.readthedocs.io/en/latest/pynwb.ecephys.html?highlight=LFP#pynwb.ecephys.LFP.create_electrical_series) method to create a new [ElectricalSeries](https://pynwb.readthedocs.io/en/latest/pynwb.ecephys.html?highlight=LFP#pynwb.ecephys.ElectricalSeries) for each tetrode, where we can store its electical data, timestamps, and the channels used to record the data.\n",
    "\n",
    "After putting the LFP data from all the tetrodes in an [LFP](https://pynwb.readthedocs.io/en/latest/pynwb.ecephys.html?highlight=LFP#pynwb.ecephys.LFP) object, we add this object to the NWBFile using the [add_acquisition()](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=Acquisition#pynwb.file.NWBFile.add_acquisition) method.  This puts it in [NWBFile.acquisition](https://pynwb.readthedocs.io/en/latest/pynwb.file.html?highlight=Acquisition#pynwb.file.NWBFile.acquisition), which is just a bucket where we put raw timeseries data like LFP. \n",
    "\n",
    "Note that we <i>don't</i> put the LFP in a ProcessingModule (located in NWBFile.modules) like we did for behavioral data.  Furthermore, there is another top-level bucket called NWBFile.analysis. Where we choose to put our datasets is a judgment call based on convention. If you are ever usure where to find something, you can always print the NWBFile to inspect its contents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing LFP data. This could take up to a minute...\\n\")\n",
    "# Get a list of all the LFP files\n",
    "eeg_path = os.path.join(data_dir, 'EEG')  # look in a subdirectory called EEG\n",
    "eeg_files = ns.get_eeg_by_day(eeg_path, animal.lower(), 'eeg')\n",
    "\n",
    "# Intialize a new ecephys.LFP object to store LFP from all of our tetrodes.\n",
    "lfp = pynwb.ecephys.LFP()\n",
    "\n",
    "# Process and store the LFP from each tetrode\n",
    "eeg_samprate = 1500.0 # Hz\n",
    "for tet_num in tetrode_metadata.keys():\n",
    "    # Give a unique name to this tetrode's LFP data\n",
    "    lfp_name = \"{prefix}eeg-{day}-{tet}\".format(prefix=animal.lower(), day=day, tet=tet_num)\n",
    "    \n",
    "    # Parse the LFP from Frank Lab Matlab files\n",
    "    timestamps, data = ns.build_day_eeg(eeg_files[day][tet_num], eeg_samprate)\n",
    "    data /= 1000 # convert from mV to V\n",
    "    timestamps += dataset_zero_time.timestamp()  # convert to POSIX time\n",
    "    \n",
    "    # Add LFP as a new ElectricalSeries in the ecephys.LFP object\n",
    "    lfp.create_electrical_series(name=lfp_name, \n",
    "                                 data=data,\n",
    "                                 electrodes=lfp_electrode_table_region[tet_num],\n",
    "                                 timestamps=timestamps)\n",
    "    \n",
    "# Add the ecephys.LFP object to the NWBFile\n",
    "nwbf.add_acquisition(lfp)\n",
    "\n",
    "print(\"Here is NWBFile.acquisition, where we put our LFP object.\\nNotice that each tetrode has a different ElectricalSeries.\\n\")\n",
    "print(nwbf.acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process clustered unit spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for processing spikes\n",
    "timestamps_column = 0  # index of the timestamps column in the units table\n",
    "NSpike_timestamps_per_sec = 10000\n",
    "\n",
    "# ------------\n",
    "# Add some metadata columns to the NWBFile.units table\n",
    "# ------------\n",
    "nwbf.add_unit_column('cluster_name',  'cluster name from clustering software')\n",
    "nwbf.add_unit_column('sorting_metric', 'a sorting metric for this unit') \n",
    "\n",
    "# ------------\n",
    "# Load spiking data into a dictionary ordered by [tetrode num][cluster num][epoch]\n",
    "# ------------\n",
    "spiking_data = ns.parse_franklab_spiking_data(data_dir, animal, day)\n",
    "\n",
    "# ------------\n",
    "# Add each cluster to the NWBFile.units table, concatenating the data across epochs\n",
    "# and keeping track of observation intervals\n",
    "# ------------\n",
    "cluster_id = 0  # increment for each cluster we process\n",
    "for tet_num in spiking_data.keys():\n",
    "    for cluster_num in spiking_data[tet_num].keys():\n",
    "    \n",
    "        # Collect spike times and obs intervals for every epoch in this cluster\n",
    "        cluster_spikes = spiking_data[tet_num][cluster_num] # dictionary indexed by epoch\n",
    "        spike_times_each_epoch = []\n",
    "        obs_int_each_epoch = np.zeros([0,2])\n",
    "        for epoch in cluster_spikes.keys():\n",
    "            # Get this cluster's spikes for this epoch\n",
    "            if cluster_spikes[epoch]['data'].shape[0]:\n",
    "                epoch_data = cluster_spikes[epoch]['data'][:, timestamps_column]\n",
    "                spike_times_each_epoch.append(epoch_data + dataset_zero_time.timestamp())\n",
    "            # Get this cluster's observation intervals for this epoch\n",
    "            for epoch_obs_int in cluster_spikes[epoch]['timerange']:\n",
    "                # Convert from Nspike timestamps to POSIX time\n",
    "                epoch_obs_int = epoch_obs_int.T.astype(float) / NSpike_timestamps_per_sec + dataset_zero_time.timestamp()\n",
    "                obs_int_each_epoch = np.append(obs_int_each_epoch, [epoch_obs_int], axis=0)\n",
    "\n",
    "        # Concatenate the spiketimes across epochs. \n",
    "        spiketimes = np.concatenate(spike_times_each_epoch)  \n",
    "        \n",
    "        # Add this cluster to the NWBFile.units table\n",
    "        cluster_name = 'd%d t%d c%d' % (day, tet_num, cluster_num)\n",
    "        nwbf.add_unit(spike_times= spiketimes,\n",
    "                      electrodes=tet_electrode_table_region[tet_num].data,\n",
    "                      electrode_group=tet_electrode_group[tet_num], \n",
    "                      obs_intervals=obs_int_each_epoch,\n",
    "                      id = cluster_id,\n",
    "                      cluster_name=cluster_name,\n",
    "                      sorting_metric=-1, # dummy value\n",
    "                     )\n",
    "        cluster_id += 1        \n",
    "\n",
    "print(\"Here is an example row from the NWBFile.units table:\")\n",
    "print(nwbf.units.to_dataframe().iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the NWBFile\n",
    "We have now added many of the core features of Frank Lab data to the NWBFile. Note that we have not used all of the available top-level fields ('analysis', 'stimulus', etc). These fields can be used to store other kinds of data as necessary for your lab and analysis pipeline. However, it is not advised to store temporary or in-progress analyses in the NWBFile, as NWB is meant to store stable versions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nwbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the NWBFile to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_filename = \"{0}{1:02}.nwb\".format(animal.lower(), day)\n",
    "with pynwb.NWBHDF5IO(nwb_filename, mode='w') as iow:\n",
    "    iow.write(nwbf)\n",
    "print('Successfully wrote NWB file: ' + nwb_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our NWBFile from disk to test round-trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pynwb.NWBHDF5IO(nwb_filename, mode='r') as ior:\n",
    "    nwbf_read = ior.read()\n",
    "    print(nwbf_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
